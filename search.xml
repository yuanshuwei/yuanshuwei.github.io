<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[singleton]]></title>
    <url>%2Fsingleton%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[常见shell]]></title>
    <url>%2Fshell-common%2F</url>
    <content type="text"><![CDATA[常见shell 文件检索: find 12345678find ~ -name &quot;build.sh&quot; # 精确查找find ~ -name &quot;build.*&quot; # 通配符find ~ -iname &quot;build.sh&quot; # 忽略大小写man find 文件内容检索: grep grep [options] pattern file 1grep &quot;moo&quot; target* # 查询包含内容&quot;moo&quot;并且文件名以target开头的文件和目标字段串所在的行的内容 grep &quot;ss&quot; ss.log 查找包含你指定内容的行，并打印你呢 grep -o 只显示想要的数据部分 grep -v 过滤掉目标数据 管道操作符 | 输出作为后面的输入 1find ~ | grep &quot;target&quot; 文件呢内容的统计: awk 12awk [options] &apos;cmd&apos; file ＃ 特别适合表格类数据，统计等awk &apos;&#123;print $1,$4&#125;&apos; netstat.txt #打印第一行和第四行, awk默认以空格分割 批量替换文本内容: sed 123sed -i &apos;s/^Str/String/&apos; xx.java #Str开头的Str替换为Stringsed -i &apos;/\.$/\;/&apos; xx.java #.结尾的替换为;sed -i &apos;/Jack/me/g&apos; xx.java ＃Jack替换成me，g是整行替换]]></content>
  </entry>
  <entry>
    <title><![CDATA[spring ioc aop]]></title>
    <url>%2Fspring-ioc%2F</url>
    <content type="text"><![CDATA[spring ioc aop spring ioc依赖注入:底层类作为参数传递给上层类，实现下层对上层的控制。依赖注入方式: Setter Interface Constructor Annotation 通过 BeanDefinition 来描述bean的定义，BeanDefinitionRegistry 的registerBeanDefinition 来注册。Bean最终存在 DefaultListableBeanFactory 中, 用ConcurrentHashMap存储的Bean名称和BeanDefinition 12Map&lt;String, BeanDefinition&gt; beanDefinitionMap = new ConcurrentHashMap(256)List&lt;String&gt; beanDefinitionNames = new ArrayList(256) BeanFactory 提供ioc的配置机制 包含Bean的各种定义，便于实例化Bean 建立Bean之间的依赖关系 Bean的生命周期的控制 BeanFactory与ApplicationContextBeanFactory是spring的基础设施，面向springApplicationContext面向使用spring的开发者 ApplicationContext继承如下接口：1EnvironmentCapable, ListableBeanFactory, HierarchicalBeanFactory, MessageSource, ApplicationEventPublisher, ResourcePatternResolver BeanFactory: 能够管理，装配Bean ResourcePatternResolver: 能够加载资源文件 MessageSource: 能够实现国际化等功能 ApplicationEventPublisher： 能够注册监听器，实现监听机制 getBean方法的代码逻辑 转换beanName 从缓存种加载实例 实例化Bean 检测parentBeanFactory 初始化依赖的Bean 创建Bean spring bean的作用域 singleton: spring的默认作用域，容器里拥有唯一的bean实例 prototype: 针对每个getBean请求，容器 request: 会为每个Http请求创建一个Bean实例 session: 会为每个session创建一个session实例 globalSession: 会为每个全局Http Session创建一个Bean实例，该作用域仅对Portlet有效 spring bean的生命周期创建 实例化bean Aware(注入Bean ID，BeanFactory和AppCtx) BeanPostProcessor(s) postProcessBeforeInitialization InitializingBean(s). afterPropertiesSet 定制的Bean init 方法 BeanPostProcessor(s). postProcessAfterInitialzation Bean初始化完毕 销毁 若实现了 DisposableBean 接口，则会调用destroy方法 若配置了 destroy-method 属性, 则会调用其配置的销毁方法 AOP实现: JdkProxy 和 Cglib 由AopProxyFactory根据AdvisedSupport对象的配置来决定 默认策略如果目标类是接口，则用JdkProxy来实现，否则用后者 JdkProxy的核心: InvocationHandler接口和Proxy类 Cglib： 以继承方式动态生成目标类的代理 JdkPeoxy是Java内部的反射机制实现的， Cglib结果ASM实现 反射机制在生成类的过程比较高效；ASM在生成类之后的执行过程比较高效 代理模式: 接口 + 真是实现类 + 代理类 spring代理模式的实现 真实实现类的逻辑包含在getBean方法里 getBean方法返回的实际是Proxy的实例 Proxy实例是spring通过JDK Proxy或CGLIB动态生成的 ACID/隔离级别/事务传播 参考这个吧 事务的传播性 name 特性 PROPAGATION_REQUIRED spring默认的机制，如果外层有事务则当前事务加入到外层事务，一块提交一块回滚，如果外层没有事务则当前开启一个新事务 PROPAGATION_REQUIRES_NEW 每次开启一个新事务，同时把外层的事务挂起，当前事务执行完毕后恢复上层事务的执行 PROPAGATION_SUPPORTS 如果外层有事务则加入该事务，如果不存在也不会新建事务 PROPAGATION_NOT_SUPPORTED 不支持事务，外层事务挂起，执行当前逻辑，恢复外层事务 PROPAGATION_NEVER 不支持事务，外层存在事务则直接抛出异常 PROPAGATION_MANDATORY 配置了的方法只能在已存在事务的方法调用，如果不存在事务的方法调用，抛出异常 PROPAGATION_NESTED 可以保存状态保存点，事务回滚时会回滚到某一个点上，从而避免所有嵌套事务都回滚]]></content>
  </entry>
  <entry>
    <title><![CDATA[redis笔记]]></title>
    <url>%2Fredis-note%2F</url>
    <content type="text"><![CDATA[redis笔记 与Memcache区别 memcache支持简单数据类型 不支持持久化 不支持主从同步 不支持分片 Redis数据类型丰富 支持持久化存储 支持主从 支持分片 Redis很快 基于内存 数据结构简单，不设计关联 单线程处理高并发，多核也可启动多实例 多路I/O复用模型，非阻塞IO IO复用模型 select系统调用 因地制宜选择IO多路复用函数 优先选择时间复杂度O(1), O(n)的select作为保底 基于react设计模式监听IO事件 常见数据类型 String: 二级制安全的 123&gt; set k v&gt; get k&gt; incr k Hash: 类似map结构 123&gt; hmset lilei name &quot;lilei&quot; age 26 title &quot;Senior&quot;&gt; hget lilei name&gt; hset lilei title &quot;Pricipal&quot; List： 有序列表 1234567&gt; lpush mylist aaa&gt; lpush mylist bbb&gt; lpush mylist ccc&gt; lrange mylist 0 101) &quot;ccc&quot;2) &quot;bbb&quot;3) &quot;aaa&quot; Set: 不重复的无序集合，哈希表时间，复杂度是O(1) 12&gt; sadd myset 111&gt; sadd myset 222 Sorted Set: 按权重排序的不重复集合, 小到大排序 1234&gt; zadd myset 3 aaa&gt; zadd myset 1 bbb&gt; zadd myset 2 ccc&gt; zrangebyscore myset 0 10 海量数据里查询某一前缀的key 摸清边界，数据量 Keys指令对于线上数据量很大，对于内存的消耗和redis服务器都是隐患 scan cursor [pattern] [count] 基于游标的迭代器，需要基于上一次的游标延续之前的迭代过程 以0开始新一次的迭代，知道命令返回游标0完成一次遍历 不保证每次执行都返回某个给定数量的元素，支持模糊查询 count只是大概的参数 1scan 0 match k1* count 10 分布式锁需要解决的问题： 互斥性: 同一时间只能一个线程获取到锁 安全性: 锁只能被持有锁的线程删除 死锁: 过期时间问题，获取到锁宕机问题 容错: redis部分宕机，需要还能继续获取到锁 SETNX 解决：1SETNX K V 不存在则创建成功，已存在则创建失败，v可以存当前线程id或者机器id；还需要设置过期时间， expire k [时间]； 但两个命令不能保证原子性；可能创建成功没设置过期时间就宕机了，此时这个锁一直不能被其他线程获取到。于是redis提供了另外的一个命令: 1set k v [EX second] [PX milliseconds] [NX|XX] EX second: 设置键的过期时间为second秒 PX milliseconds： 设置键的过期时间是毫秒 NX： 键不存在时，才对键进行设置操作 Redis异步队列12345&gt; rpush mylist aaa&gt; rpush mylist bbb&gt; rpush mylist ccc&gt; lpop mylist&gt; aaa 缺点: 没有等待队列里有值就直接消费 弥补: 可以在应用层引入sleep机制去调用LPOP重试 不用sleep的话，直接用BLPOP，没有消息的时间就阻塞，超时后再返回。 blpop testlist [超时时间] 主题订阅模式， pub/sub 发生消息，接收消息123&gt; subscribe mytopic&gt; subscribe mytopic&gt; publish mytopic &quot;hello&quot; 这种无法保证可达，会丢失。 数据持久化 RDB: 快照,每隔一段时间保存当前全量数据，多个文件；速度快文件小，相对容易丢失 AOF: 保存所有写命令,保存命令；数据不易丢失。文件体积大，恢复时间长 RDB-AOF混合 持久化方式(redis4.0后)： RDB保存全量数据-AOF保存增量数据 同时存在AOF和RDB文件，的数据恢复： 重启redis即可。优先加载AOF文件 ### pipeline 与linux的管道类似 redis基于请求响应模式，单个请求处理需要一一应答 pipeline批量执行指令，节省多次IO往返时间 主从复制主从同步，从从同步。 全量同步 slave发送sync命令到master master启动一个后台进程,将redis的数据快照保存在文件中 master把文件发送给slave，slave对AOF文件加载 增量同步 master的写命令存在AOF中, 将命令数据写入缓存然后发生给slave 整理 作用:高性能，高并发； 雪崩: 宕机的处理 事前：主从复制＋哨兵，redis cluster维持高可用 事中，限流，降级，增加本地缓存 事后: redis持久化，快速恢复数据 redis线程模型: 它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。多个socket,IO多路复用，文件时间分派器，事件处理器 redis效率高: 纯内存操作,IO多路复用的非阻塞IO,Ｃ语言实现,单线程避免上下文切换问题,单线程处理高并发，多核也可启动多实例 数据类型: string,hash,list,set,sorted sort 过期策略: 定时删除(默认100ms抽一部分有过期时间的key)＋惰性删除(用到时检查)＋内存淘汰机制(常用LRU, 空间不足时，删除最近最少使用的key) 持久化方式: RDB和AOF,RDB周期性保存全量的数据备份，AOF保存全部的写命令。 RDB适合做冷备份，对外读写服务影响小，保持高性能。但每隔5分钟甚至更长会导致丢失数据。 对于AOF：每个１秒执行，适合误删除的紧急恢复，同一份数据AOF更大，恢复较慢，命令回访恢复数据，不够健壮。 一般的方式:综合两种方式，AOF保证数据的不丢失，作为第一选择；RDB做不同程度的冷备份。 主从复制: 第一次从连接主时: 从发送命令PSYNC,主生成RDB文件发送给从，并在内存中缓存RDB生成后的所有写操作，当从加载完RDB文件，主将缓存中的写命令发送给从,完成数据的同步。 主从复制断点续传: redis2.8以后，主节点在内存中维护一个backlog,主和从都会保存一个replica offse还有一个run id,offset就是保存在backlog中,如果主从断开连接，从会让主从上次replica offset继续复制 哨兵: 作用: 集群监控(监控主从节点是否正常)、消息通知(每个redis故障则通知给管理员)、故障转移(主从切换)、配置中心(故障发生，通知cilent客端新的主节点的地址)。哨兵本身也是分布式的。 自动发现机制: 哨兵通过sub/pub,每个哨兵都向__sentinel__:hello这个channel发消息，同步自己的host,ip,runid还有对master的监控。来感知其他哨兵的存在，互相同步各自监控信息和配置信息。 主从切换步骤: 每个哨兵ping master,超过指定时间就认为宕机，此时是主观宕机；当哨兵集群中超过一半都认为master宕机了，此时到达了客观宕机，可以进行主从切换 选举算法: ①跟master断开时长②slave优先级③复制offset(复制数据更多的那个)④run id(大小排序) 集群(redis cluster): 16384个槽，哪个master负责哪个槽用户可以指定。所有键都会被直派到具体的槽:CRC64(key)%16384,也就是key的校验和对16384取余得到 每个节点有自己看来整个集群的状态，包括某个槽对应哪个节点 当客户端向集群中任一节点发送与数据库键有关的命令时，接收命令的节点会计算出命令要处理的数据库键属于哪个槽（CRC16(key) &amp; 16383），并检查这个槽是否指派给了自己： ·如果键所在的槽正好就指派给了当前节点，那么节点直接执行这个命令。 ·如果键所在的槽并没有指派给当前节点，那么节点会向客户端返回一个MOVED错误，指引客户端转向（redirect）至正确的节点，并再次发送之前想要执行的命令。 图解redis集群原理 原理 原理2 手写LRU 参考offse还有一个run id,offset 1234567891011121314151617181920class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private final int CACHE_SIZE; /** * 传递进来最多能缓存多少数据 * * @param cacheSize 缓存大小 */ public LRUCache(int cacheSize) &#123; // true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最老访问的放在尾部。 super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true); CACHE_SIZE = cacheSize; &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123; // 当 map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据。 return size() &gt; CACHE_SIZE; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[mysql笔记]]></title>
    <url>%2Fmysql-note%2F</url>
    <content type="text"><![CDATA[mysql笔记 mysql基础架构 Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 日志系统redo log(innoDB特有) 当一条记录需要更新的时候，innoDB先把记录写到redo log，并更新内存，这个时候更新就算完成了，innoDB会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是系统空闲的时候 redo log是固定大小的，从头开始写，写到末尾又回到开头循环写。在这个环上一个标记写位置的指针，一个标记当前擦除位置的指针，当快追上的时候就开始移动指针删除数据。 有了redo log，就可以保证数据库发生异常重启，之前的记录不会丢失，这个能力称为crash-safe binlog(归档日志) mysql server层实现的 redo log是物理日志， 两段提交 写入redo log处理prepare阶段 写入bin log 提交事务，处于commit状态 为什么两段提交？使得bin log与 redo log一致redo log: 少一条的话，更新数据库重启的恢复bin log: 影响数据库备份的数据 事务 事务特性: (ACID) 原子性(undo log 回滚)、一致性(两个操作数据一致性)、隔离性、持久性(修改不因数据库重启而丢失，redo log) 多事务同时执行，可能出现的问题: 脏读、不可重复读、幻读 事务隔离级别(多个事务执行间的问题): 读未提交、读提交、可重复读、串行化 不同隔离级别的区别: 读未提交: 一个事务还未提交，他所做的修改可以被其他事务看到(没有视图概念，没解决任何并发问题) 读提交: 一个事务提交以后，他所做的修改才可以被其他事务看到(每个语句执行时创建的视图，解决了脏读问题) 可重复读: 一个事务执行过程看到的数据是一致的(视图实现),未提交的更改对其他事务不可见。(事务启动时创建视图实现，解决不可重复读问题) 串行化: 对应一个记录加读写锁，出现冲突时，后访问的事务必须等前一个事务执行完成才能继续执行(加锁实现，解决所有问题) 配置方法: 启动参数transaction-isolation 事务隔离的实现: 每条记录更新时都同时记录一个回滚操作，同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制(MVCC) 回滚日志什么时候删除: 系统判断当没有事务需要用到这些回滚日志的时候，回滚日志会被删除 什么时候不需要了？当系统没有比这个回滚日志更早的read-view的时候 为什么尽量不要使用长事务？长事务意味在系统里存在很老的事务视图，事务提交之前都会保存，导致占用大量存储空间，长事务还占用锁资源，可能拖垮库 多事务同时执行可能出现的问题 参考 脏读: 一个事务修改了数据还未提交，另一个事务读取并使用了这个数据。当第一个事务回滚，另一个事务使用的数据就是脏数据(发生在读未提交) （由于其他事务没有提交造成） 不可重复读: 一个事务内多次读同一个数据，两次读之间数据被另一个事务修改，导致这两次读到的数据不一致。（由于其他事务提交造成） 幻读: 事务1对所有数据行执行修改，同时事务2添加了一条数据。事务1发现自己并没有实现全部数据的修改，好像发生了幻觉。(解决: 一个事务完成了处理前，其他事务不可以插入数据) （需要插入） mysql默认是可重复读，oracle默认是读提交 “autocommit”为事务的自动提交，设置为”off”时需要手动，开启和提交事务123begin # 或 start Transactionupdate xxxxcommit # rollback 事务结束的时候，所持有的锁会释放 索引数据结构的考虑: 哈希表:类似数组+链表，key的hash值确定在数组(桶)中的位置，出现hash冲突时，拉链。但这种对于范围查询不适用，需要全部扫描，比较适合等值查询 有序数组:查询利用二分查询,时间复杂度是O(log(N)); 也适用范围查询。但插入元素，需要移动后面的全部元素，效率很低。适合静态存储引擎，不会修改数据了 二叉树: 特点是，每个节点的左二子小于这个节点，右二子大于当前节点；这样查询的效率是O(log(N)),为了维持这个查询复杂度，需要保持这棵树是平衡二叉树，所以更新时间复杂度也是O(log(N))。 实际索引使用的是多叉树(一个树有多个节点),原因是索引需要写磁盘，尽量要减少数据块的寻址，让树高小一点。 N叉树的N取决于数据块的大小。 跳表:redis中用于代替平衡树的结构，插入/删除/搜索都是O(log(n))。一种双向链表的发展，多层指向下一节点的指针。 LSM:相比下，B+树是随机磁盘访问，比顺序读写要慢。LSM顺序写，牺牲了一些读性能 索引字段问题 主键索引(聚簇索引，叶子节点存的是整行数据)、非主键索引(二级索引，叶子节点存的是主键，所以还需要再查询一次主键索引的树)；我们应尽量使用主键查询。 而考虑占用空间，比如要用身份证号还是自增id做主键，显然主键越小，普通索引的叶子节点就越小，普通索引的占用空间也越小，所以自增主键更适合。 索引定位到page，page内部是一个有序数组，通过二分法查询。 通过普通索引查询到主键，再去主键索引树查询,叫做回表。 自增主键的选择 选择的场景: 性能: 保持B+树的有序性，自增可减少页分裂和合并。 空间: 比如和身份证号做主键相比，因为非主键索引存的值是主键，主键较大，会比较占用存储空间。 不选择的场景:直接使用业务字段来作为主键: 只有一个索引且必须是唯一索引。 死锁 事务A是有行1的锁，去申请行2的锁；事务B持有行2的锁，去申请行1的锁 持有对方需要的锁，并请求对应资源。解决: 执行死锁的回滚事务。对于InnoDB将持有最少行级排他锁的事务回滚 一些索引 覆盖索引: 如果查询条件使用的是普通索引(或者是联合索引的最左原则)，查询结果是联合索引的字段或主键，不用回表，直接返回结果，减少IO磁盘读写读取正行数据。 最左前缀: 联合索引的最左N个字段，也可以是字符串索引的最左M个字符。 联合索引: 根据创建联合索引的顺序，以最左原则进行where检索，比如(age,name)以age=1或者age=1 and name=’张三’可以使用索引，单以name=’张三’不会使用索引，考虑存储空间问题，需要根据业务需求，将查找频繁的数据进行靠左创建索引。 索引下推: like ‘hello%’ and age &gt;10检索，mysql5.6之前，会对匹配的数据进行回表查询，5.6以后，回先过滤掉age&lt;10的数据，再进行回表查询，减少回表率，提升检索速度。 为什么用B+存储索引?(演变过程) 参考 hash: 查询很快，适合固定值查询。不适合范围查询。 有序数组: 查询和范围查询都很快，插入需要移动后面全部元素，效率较低。 二叉搜索树: 存在极端情况，出现链表结构，搜索效率较低 平衡二叉树: 解决了上面的问题，但对于索引大部分节点存在磁盘，每个节点是非连续的空间，每个节点的寻址操作是影响搜索速度的关键。基于此尽量需要减小树的高度 B树与B+树: 每个节点多放一些数据，对应数据库读取的最小单位页 B+树跟B树不同，B+树叶子节点冗余了所有非叶子节点的数据 B+树每个叶子节点增加了指向相邻的节点的指针 优点: 非叶子节点不会带上指向记录的指针，这样，一个块中可以容纳更多的索引项，一是可以降低树的高度。二是一个内部节点可以定位更多的叶子节点 叶子节点间有序的链表，有利于范围查找。 哪些方式无法应用到索引 参考 ①函数 ②非左匹配 like &#39;%5400%&#39;; like &#39;5400%&#39;是可以的 ③in , not in ④&gt;, &lt;, is null, is not null ⑤UNION 全局锁、表级锁、行级锁全局锁: 读整个数据库实例加锁。 mysql提供加锁全局读锁的方法:Flush tables with read lock(FTWRL)这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句会被阻塞 使用场景: 全库逻辑备份 风险: 如果在主库备份，在备份期间不能更新，业务停摆 如果在从库备份，备份期间不能同步binlog，导致主从延迟，官方自带逻辑备份工具mysqldump,当mysqldump使用参数–single-transaction的时候，会启动一个事务，确保拿到一致性视图，而由于MVCC的支持，这个过程中数据是可以正常更新的 一致性读是好，但前提是引擎要支持隔离级别 如果全库只读为什么不使用set global readonly=true的方式？ readonly在有些系统被用来其他逻辑，比如判断主备库，所以修改的话影响较大 异常机制有差异，FTWRL在发生异常时，会释放全局锁，readonly一直保持 表级锁 mysql有两种表级锁:一种是表锁；一种是元数据锁(meta data lock, MDL) MDL:不需要显示使用，访问一个表时字段加上 读写锁互斥，写写锁互斥，读读锁不互斥 MDL会直到事务提交才会释放，在做表结构变更是，一定要小心不要导致阻塞线上查询和更新 表级锁限制只能当前线程的操作，比如，线程A执行lock t1 read,t2 write ，则其他线程写t1。读写t2都会被阻塞 表锁分为：意向共享锁和意向排他锁，他们是一种锁的标志，为了使加表锁效率高，因为有存在任何行锁就不能加表锁，全表扫描有没有锁效率很低；这种意向锁是存在任何锁时，更新这个标志 共享锁（行锁） 又称为读锁，S锁，多个是事务对与同一数据共享一把锁，都只能访问到数据，只能读不能修改 加锁释放锁方式:select * from student where id =1 LOCK IN SHARE MODEcommit/rollback 排他锁(行锁) 又称为写锁，X锁，不能与其他锁并存，如一个事务获取了一个数据行的 排他锁，其他事务不能获取该行的锁(共享锁，排他锁)，只有获取了该排他锁的事务可以对数据行读和修改 加锁释放锁方式L:自动: delete / update / insert 默认加X锁手动: select * from student where id=1 FOR UPDATEcommit / rollback 乐观锁与悲观锁 参考 悲观锁: 假设一定会发生冲突，所以每次拿数据之前都要加锁 (select for update 锁定当前行) 乐观锁: 每次拿数据都认为别人不会修改，只在提交时检查数据是不是已经被别人更新过了，乐观锁适用于读多写少的场景，这样可以提高吞吐量。(cas方式,字段值没有被其他线程修改，则更新操作) 版本字段控制: 增加一个数字类型的字段’version’,每次更新加1。 CAS方式实现 使用时间戳，CAS，比较时间 悲观锁和乐观锁是用法，而不是划分的锁类型 锁住了什么，行数据 / 列？ 没有用到索引时，排他锁，直接锁表，为什么？没有创建索引时，存在一个默认索引，扫描索引就所住了整张表 锁住的是索引 记录锁锁住的是该记录，where id = 4, 锁住的是id=4那一条记录(唯一性索引，等值查询，精确匹配) 间隙锁(只在可重复读隔离级别中)，左开右开区间，比如1，4，7，10 。(-,1),(1,4),(4,7)…对于等值查询没有命中和范围查询，锁住对应的范围；间隙锁间不互斥 临键锁，左开右闭区间 分库分表分表 单表数据量太大，影响sql执行性能 分库 并发量很高，单库并发量最好控制在每秒1000左右 中间件推荐 Sharding-jdbc， 当当开源，client层方案 Mycat，基于阿里的Cobar改造，proxy层解决方案 水平拆分与垂直拆分 水平: 就是把一个库的一个表数据弄到多个库的多个表中去，每个库的表结构一样，只不过每个表数据不同 垂直拆分: 表级别，将表字段拆分到不同的表中，单表最好控制在200万左右 两种分库分表方式 按range，比如时间范围,但会有近期热点访问问题 按每个字段，比如hash一下，分布均匀；会有扩容时的重新hash问题 主从复制 从库到主库来去binlog，然后在本地执行sql语句 半同步复制: 主库写入binlog以后，强制立即给从库，从库写完，至少一个返回ack，才算写成功 主从同步延迟问题 分库，降低主库的并发 重写代码，尽量不要写完马上查询]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019面经]]></title>
    <url>%2Finterview-experience-2019%2F</url>
    <content type="text"><![CDATA[面经整理～～ 58金融 另一份 微服务 谈谈你理解的微服务 什么是微服务—微服务的来龙去脉—什么是微服务架构 服务的划分依据 一次dubbo的完整调用过程 [1] – [2] mysql 千万级别的数据，数据库需要考虑什么 [1] — [2] 数据库分库分表需要注意什么 [1]— [2] 索引需要考虑什么 最左原则 说说B+树和B树 讲讲幻读和不可重复读 讲讲数据库的乐观锁和悲观锁 redis redis持久化 队列 kafka为什么比mq快–kafka为什么那么快 生产者: 顺序写入、内存映射文件(同步或异步更新到磁盘) 消费者: 全部以文件方式发送到消费者 zookeeper 分布式锁 zookeeper选举策略，双数节点可以选举成功吗 HashMap HashMap结构，为什么是数量是8转为红黑树(总结) 说说红黑树 怎么保证HashMap线程安全 线程 线程生命周期，什么时候发生线程状态变为死亡–线程的生命周期 讲讲CAS和AQS ThreadLocal结构，与线程池一起使用会有什么问题吗？ 线程池中的线程在任务执行完成后会被复用，所以在线程执行完成时，要对 ThreadLocal 进行清理（清除掉与本线程相关联的 value 对象）。不然，被复用的线程去执行新的任务时会使用被上一个线程操作过的 value 对象，从而产生不符合预期的结果。 jvm 讲讲你熟悉的GC收集器，特点，你们用的哪个 jvm调优怎么做 排序 快排 瓜子第一轮 coding: 文件夹后续遍历: 先打印文件然后追层向上打印 无序数组，取出top N的N个元素 ConcurrentHashMap 在JDK8的优化 jvm内存结构 第二轮 消息中间件的作用 并发 为什么产生锁 threadlocal源码 数据库 mysql 索引 redis 雪崩 击穿 jvm 内存划分 新建对象放在哪里 (堆和栈) G1收集器 增加吞吐量需要怎么设置参数 jvm调优 优化分析 接口变慢可能是什么原因？怎么优化? dubbo的限流 分布式 接口幂等的实现方式 分布式锁 洋钱罐 简单的sql手写(join) mysql事务隔离级别 redis怎么用的算法手写二叉树根节点到叶子的数据和的最大值(广度优先,没一级存当前与上面的和，叶子节点就是总和) vipkid 项目遇到的难点 redis cluster写热点key问题 一个线程池，3个线程执行完成就继续执行下去，写代码(CountdownLatch) 算法手写 字符串反转 和为n的连续正整数序列 单链表反转 等等 转转zk分布式锁的临时节点如何释放的select for update是什么锁 12select from where id = &apos;&apos;select from where name = &apos;&apos; 行锁还是表锁？触发不到索引的查询有哪些水平分表,同时满足订单id,卖家id和卖家id分别查询dubbo为什么用代理(方法的多态,消费组)线程池参数算法 用map, list, array, stack 实现LRU cache 反转单链表 不用Java Api实现字符串转整型 足够量大的动物的动物园有一种动物是一只的，其他种类是两只的，找出这种动物 知乎 spring aop ioc, 动态代理中jdk和clib的区别 redis线程模型为什么快(NIO) redis分布式锁,多线程，某个线程阻塞超时，重新执行时锁已被其他获取，自己删除别人的这个锁(怎么解决标记锁所属线程，保证原子性) dubbo的执行过程, 代理监听和执行这中间做了什么 HashMap为什么线程不安全(rehash时链表成环) ConcurrentHashMap怎么做到的分段锁，怎么得到的当前的大小 线程池参数 AQS怎么做的线程放入队列，释放锁唤醒队列线程(源码细节) CAS mysql事务隔离级别 索引结构， B+树数据全部存在叶子节点的好处(B树也可以把数据连起来啊) 回表,覆盖索引 新浪金融算法 leetcode 821题 斐波纳契数列(非递归实现) 二分查找 ArrayList删除指定元素(考虑越界问题) 懒加载单例 java内存模型，垃圾回收策略，jvm调优(堆为什么也不能设置很大？)Spring AOP, IOC原理？ JDK动态代理和cglib的区别？ Spring事务嵌套？Mybatis如何防止SQL注入的dubbo生产者和消费者与zk的流程，负载均衡是在哪配置的？设计模式有哪些事务的特性ACIDHashMap的存储结构，put的操作过程sleep和wait的区别synchronized用在不同方法的情况分布式锁讲讲你熟悉的框架，技术 马蜂窝 字节码加载到卸载的全过程 新生代不够的情况，对象怎么分配 现在jvm内存设置的多少 业务的并发量(吞吐量) 并发用到过哪些地方 mysql的索引 B与B+树 聚簇索引和非聚簇索引 京东一面 http报文 Http协议有哪些 传输 kafka发送文件方式（同步，异步，批量） kafka架构 Kafka遇到的问题解决 什么场景用的消息队列 Redis用过哪些数据结构 Dubbo的调用过程 Dubbo通信协议，连接协议 集合用过哪些 Hashmap与concurrenthashmap 锁有哪种，共享，独享 哪些类是共享，独享 Mysql索引考虑什么 什么情况应用不到索引 表锁行锁全局锁，怎么加 限流怎么做的，原理 本地锁与分布式锁 Zk与redis实现的区别 jvm参数调优 哪些情况cpu升高 什么情况出现线程安全问题 内存划分 垃圾回收 用的什么回收器，为什么，特点 你们系统QPS多少，用户量 final变量为什么在使用前必须要进行初始化 二面和三面 dubbo调用其他系统时，被调用系统挂掉如何不影响调用方（降级） dubbo调用过程经过zookeeper吗，一个机器挂了，没有更新的调用方本地缓存，怎么办？ dubbo限流有没有做过？ mysql redis mongo使用场景 mongo的锁问题有没有遇到过？（数据量大时出现） redis的数据结构，一个班学生用什么结构存 innodb和myisam区别 用过哪些索引 索引创建需要考虑哪些 关于简历里的授权系统和联合登陆 zookeeper和redis分布式锁，量大时zk文件节点的创建和销毁性能与redis重复尝试加锁的性能消耗？ 第四面 遇到过那些难题 Mysql遍历量很大的数据，怎么查 这家公司学到了什么 dubbo原理，具体执行过程都做了什么]]></content>
  </entry>
  <entry>
    <title><![CDATA[网络基础知识]]></title>
    <url>%2Fnetwork-base%2F</url>
    <content type="text"><![CDATA[网络基础知识整理 OSI七层模型 物理层(网卡): 比特流; 二级制-&gt;电流强弱-&gt;二进制 (数模, 模数 转换) 数据链路层(交换机): 帧; 格式化数据，错误检测和纠正 网络层(路由器): 分组数据报; 网络地址转换为物理地址，并决定如何发送到接收方，路径；IP协议 传输层: 分段； 拆分为多个数据包segment, 决定传输速率，顺序；TCP协议 会话层: 管理应用程序自动发包，寻址。不同机器上 表示层: 不同系统通信语法解决，格式化 应用层: 规定消息头格式，消息体。解读信息 TCP/IP(OSI的实现) TCP 面向连接、可靠的、基于字节流的传输层通信协议 将应用层数据流分割成报文段并发送给目标节点TCP层 数据包有序号，发ACK确认 使用校验和检验数据在传输过程 TCP 标志 URG: 紧急指针标志 ACK: 确认序号标志 PSH: push标志 RST: 重置连接标志 SYN: 同步序号，用于建立连接过程 FIN: finish标志, 用于释放连接 TCP三个握手 ①建立连接时，客户端发送SYN包到服务器，并进去 SYN_SEND 状态，等待服务器确认。 SYN=1, seq=x。 ②服务器收到SYN包，必须确认客户的SYN(ack=j+1)，同时自己也发送一个SYN包(syn=k)， 此时服务器进入SYN_RECV 状态； SYN=1, ACK=1, seq=y, ack=x+1。 ③客户端收到服务器 SYN+ACK 包，向服务器发送确认包ACK(ack=k+1),此包发送完毕，客户端和服务器进入连接状态；ACK=1，seq=x+1, ack=y+1。 为什么是三次握手?为初始化Sequence Number的初始值]]></content>
  </entry>
  <entry>
    <title><![CDATA[apollo实时更新问题]]></title>
    <url>%2Fapollo-refresh%2F</url>
    <content type="text"><![CDATA[大概介绍分布式配置, 统一管理不同环境，不同集群的配置。 配置修改时服务端实时推送到客户端，客户端数据存在内存(不用自己管理),并缓存在本地文件中；同时客户端一段时间(默认5分钟)也会主动去服务端拉数据，防止推送失败 分为configService、adminService、portalService 部署文档、使用文档 问题配置的更新推送，更新的是本地内存, 我们希望接收到更新到spring context中去。甚至重新初始化bean，重新建立数据库连接等，做到通过apollo集中管理配置更新，无需程序重新启动的完成配置更新。 实现示例① apollo提供了changeListner监听事件，可以在监听到更新后，对已有的Properties的属性完成更新1234567ConfigService.getAppConfig().addChangeListener(changeEvent -&gt; &#123; System.out.println(&quot;Changes for namespace &quot; + changeEvent.getNamespace()); for (String key : changeEvent.changedKeys()) &#123; ConfigChange change = changeEvent.getChange(key); System.out.println(String.format(&quot;Found change - key: %s, oldValue: %s, newValue: %s, changeType: %s&quot;, change.getPropertyName(), change.getOldValue(), change.getNewValue(), change.getChangeType())); &#125; &#125;); ② 将对于的更改更新到spring中去，Spring cloud 提供了RefreshScope，示例演示@ConfigurationProperties配置的bean更新 12345@Datapublic class Testconfig &#123; private String name; private int age;&#125; 123456789101112public class App &#123; @Bean @ConfigurationProperties(prefix = &quot;test&quot;) public Testconfig testconfig() &#123; return new Testconfig(); &#125; @Bean public SpringBootApolloRefreshConfig springBootApolloRefreshConfig() &#123; return new SpringBootApolloRefreshConfig(); &#125;&#125; 123456789101112131415161718192021222324@Componentpublic class SpringBootApolloRefreshConfig &#123; private static final Logger logger = LoggerFactory.getLogger(SpringBootApolloRefreshConfig.class); @Autowired private Testconfig testconfig; @Autowired private RefreshScope refreshScope; @Autowired private ContextRefresher contextRefresher; public SpringBootApolloRefreshConfig() &#123; &#125; @ApolloConfigChangeListener(value = ConfigConsts.NAMESPACE_APPLICATION) public void onChange(ConfigChangeEvent configChangeEvent) &#123; logger.info(&quot;before refresh: &#123;&#125;&quot;, testconfig.toString()); refreshScope.refreshAll(); contextRefresher.refresh(); logger.info(&quot;after refresh: &#123;&#125;&quot;, testconfig.toString()); &#125;&#125; ③ 完成如dataSource, 更新配置后，完成新建数据库连接，新的请求转到新的连接，旧连接逐步销毁 // TODO参考这个讨论]]></content>
      <tags>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件面试常见问题]]></title>
    <url>%2Fmessage-middleware%2F</url>
    <content type="text"><![CDATA[消息中间件面试常见问题, 整理自github 一、为什么用消息队列? 解耦(发布订阅模型)、异步(速度快)、削峰(从mq拉的速度控制一下) 可用性降低(mq挂)、系统复杂度高(消息重复、消息丢失、消息顺序)、一致性问题(多个系统消费可能存在失败的) 结合项目体现 二、ActiveMQ、RabbitMQ、RocketMQ、Kafka优缺点，如何选型 ActiveMQ社区不活跃，没经过大规模吞吐场景验证，不推荐 RabbitMQ，erlang开发，阻止了大多人深入底层，中小公司推荐；社区活跃不会黄 RocketMQ，适合大公司，自己有实力 Kafka，大数据实时计算，日志采集的业界标准 三、如何保证消息队列的高可用？ Kafka, 架构认识：由多个Broker组成，创建一个topic可以划分为多个partition，每个partition存在不同的broker里。就是说每个topic的数据存在不同的机器中，3个partition就是3个机器中 0.8以前，如果一个实例挂，就相当于topic丢了一部分数据。之后加入了replica副本，partition会存在副本；自动选举leader和follower； 读写全部从leader节点；写数据的时候，leader存本地磁盘，follower主动去leader拉，全部follower拉完会发ack给leader，再返回给生产者；读数据时，只有所有follower都返回ack同步成功的才会被消费 四、 消息队列的重复消费、幂等性？ 都可能出现重复消费，这是开发来保证的不是MQ 对于kafka，有一个offset概念，代表消息的序号，每隔一段时间消费者会把消费过的offset提交，表示已经消费过了，但异常情况来不及提交，会重复消费，所以需要我们开发自己来保证幂等性 五、如何保证可靠性、消息丢失？ Kafka 消费端丢失: 消费了未处理，就自动提交了offset；此情况可以关闭自动提交，处理完成后才手动提交 Kafka丢失: 某个Broker中leader挂了，还未来得及同步partition，进行了选举；所以配置一些参数: 保证每个partition至少有两个副本: topic的 replication.factor 大于1 leader感知至少有一个follower存在: kafka设置min.insync.replicas大于1 写入所有的replica,才认为是成功: producer设置 acks=all 一旦写入失败无限重试: producer设置 retries=MAX 生产者不会丢失，因为所有到接收到才算成功 六、消息消费的顺序性? 保证一个queue或topic，或同一类的消息由同一个消费者消费 参见 七、延时过期失效、队列满了、消息积压几个小时，怎么解决？这里 就是消费端出现了问题 八、设计消息队列的思路 可伸缩，动态扩容： 参考kafka，每个机器存一部分数据，如果资源不够了，给topic增加partition，然后做数据迁移，增加机器，就可以存放更多数据 数据持久化： 顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是 kafka 的思路。 可用性： 参考之前的，多副本，leader&amp;follower-&gt;broker, 挂了就重新选举 数据零丢失: 之前的方案 这里]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过nginx自建文件共享服务，用手机播放电脑的视频]]></title>
    <url>%2Fnginx-file-share%2F</url>
    <content type="text"><![CDATA[背景 最近想把一些学习视频共享给小伙伴，结果发现网盘生成链接很容易失效，下载限速严重；加上手机内存有限观看下载好的视频很不方便；所以觉得搞一下本地文件服务器，来分享文件；同时手机也可以连接在线观看视频，因为是内网，速度拉满 环境mac(windows有乱码问题)安装好nginx 配置nginx.conf1234567891011121314http &#123; server &#123; charset utf-8,gbk; # windows 服务器下设置后，依然乱码，暂时无解 listen 80 default_server; listen [::]:80 default_server; server_name _; location / &#123; root /Users/yuan/sp; #共享目录，Windows目录这样写 d://www// autoindex on; ##显示索引 autoindex_exact_size on; ##显示大小 autoindex_localtime on; ##显示时间 &#125; &#125;&#125; 注:查看默认nginx配置文件位置执行: nginx -h, 如下可知，-c对应的/usr/local/etc/nginx/nginx.conf，即是当前默认配置文件123456789101112131415yuandeMacBook-Pro-2:nginx yuan$ nginx -hnginx version: nginx/1.15.3Usage: nginx [-?hvVtTq] [-s signal] [-c filename] [-p prefix] [-g directives]Options: -?,-h : this help -v : show version and exit -V : show version and configure options then exit -t : test configuration and exit -T : test configuration, dump it and exit -q : suppress non-error messages during configuration testing -s signal : send signal to a master process: stop, quit, reopen, reload -p prefix : set prefix path (default: /usr/local/Cellar/nginx/1.15.3/) -c filename : set configuration file (default: /usr/local/etc/nginx/nginx.conf) -g directives : set global directives out of configuration file 使用启动nginx访问http://127.0.0.1即可，手机访问更换为电脑ip 参考nginx命令及教程推荐参考 Nginx极简教程 自建视频网站]]></content>
      <categories>
        <category>好玩</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo 站内文章引用]]></title>
    <url>%2FHexo-internal-link%2F</url>
    <content type="text"><![CDATA[格式如下, 链接文字可省略1&#123;% post_link 文件名 链接文字 %&#125; 如: 链接到文件: hello.md1&#123;% post_link hello %&#125; 1&#123;% post_link hello 这是指向hello的链接 %&#125;]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal源码分析]]></title>
    <url>%2Fthreadlocal%2F</url>
    <content type="text"><![CDATA[前言以前接触过ThreadlLocal，不过只是一点点并且没有及时记录下来，最近看了一些博客并结合源码，了解了一番，很有必要记录一下。欢迎指正~ 一、简单介绍ThreadLocal可以存储当前线程下的数据，并保持各线程的数据互不干扰。方便使用，并减少传输。使用起来类似HashMap的key，value形式，set和get，不过同一个ThreadLocal对象只能存储一个值 二、一般使用123456789101112131415161718public class Test &#123; // 一般以private static方式定义 private static ThreadLocal&lt;String&gt; threadLocal; private static ThreadLocal&lt;String&gt; threadLocal2; public static void main(String[] args) &#123; threadLocal = new ThreadLocal&lt;&gt;(); threadLocal2 = new ThreadLocal&lt;&gt;(); threadLocal.set("abc"); threadLocal2.set("bcd"); System.out.println(threadLocal.get()); System.out.println(threadLocal2.get()); &#125;&#125; 整体结构: Thread下存ThreadLocalMap-&gt;Entry数组-&gt;数组的下标通过key(当前ThreadLocal对象)的hash函数确定，Entry对象存在key和value属性 三、源码分析 来自: JDK1.8 1234567891011121314151617181920212223242526272829303132333435363738394041public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125;// ThreadLocalMap.set()private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash(); &#125; 通过上面代码发现，set的过程是: 首先获取当前线程Thread对象的ThreadLocalMap引用 getMap(t) 的 t.threadLocals 然后对ThreadLocalMap引用进行 map.set() ThreadLocal对象作为key将值存到ThreadLocalMap下的Entry数组中(类似HashMap，它是通过hash与数组长度减1与，离散的存到数组的不同索引位置，不同的是，由于不是链表的结构，需要判断即将插入的位置和当前元素的关系，来调整位置) 注:ThreadLocalMap是ThreadLocal的内部类，如下123456789101112static class ThreadLocalMap &#123; static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; ...&#125; 从上面又可以看出，ThreadLocalMap是在ThreadLocal中使用内部类来编写的，但对象的引用是在Thread中！ 于是可以总结出：Thread为每个线程维护了ThreadLocalMap这么一个Map，而ThreadLocalMap的key是LocalThread对象本身，value则是要存储的对象 下面再来看get()就不难了12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 四、内存泄露123456789static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; Entry的key保存在外部的 WeakReference，弱引用，ThreadLocal在没有外部强引用时，可以被GC回收 但如果线程一直在运行，这个Entry的value一直不能被回收，强引用，会发生内存泄露;Entry是key的弱引用，不是实例对象value的弱引用，无法避免内存泄露 为了尽量避免泄露，在不需要的时候，执行 remove() 方法 引用:Java WeakReference的理解与使用理解Java的强引用、软引用、弱引用和虚引用 remove做了什么呢?12345678910111213141516171819202122 /** * Remove the entry for key. */ private void remove(ThreadLocal&lt;?&gt; key) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; if (e.get() == key) &#123; e.clear(); expungeStaleEntry(i); return; &#125; &#125; &#125;// Reference.clear()public void clear() &#123; this.referent = null;&#125; clear() 将key置null, 然后 expungeStaleEntry(i) 将Entry数组重新组合，遍历数组，key或value为空的对象置空 五、总结 存储当前线程可访问的数据，减少数据传输 private static ThreadLocal&lt;T&gt; local 因为存储的值在ThreadLocalMap中，与每个线程绑定，所以各个线程间的访问互不影响 数据通过它的内部类ThreadLocalMap，存储在ThreadLocalMap.Entry中，最终是存储在每个线程Thread中的ThreadLocalMap引用 调用ThreadLocal的set()方法时，实际上就是往ThreadLocalMap设置值，key是ThreadLocal对象，值是传递进来的对象 调用ThreadLocal的get()方法时，实际上就是往ThreadLocalMap获取值，key是ThreadLocal对象 ThreadLocal本身并不存储值，它只是作为一个key来让线程从ThreadLocalMap获取value。 ThreadLocal设计的目的就是为了能够在当前线程中有属于自己的变量，并不是为了解决并发或者共享变量的问题]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8笔记]]></title>
    <url>%2FJava8-note%2F</url>
    <content type="text"><![CDATA[主要是Optional和Stream的基本用法 Optional逐层取值1234567891011String version = "UNKNOWN";if(computer != null) &#123; Soundcard soundcard = computer.getSoundcard(); if(soundcard != null)&#123; USB usb = soundcard.getUSB(); if(usb != null)&#123; version = usb.getVersion(); &#125; &#125; &#125; 改进: flatMap返回Optional对象, map提取属性值1234String name = computer.flatMap(Computer::getSoundcard) .flatMap(Soundcard::getUSB) .map(USB::getVersion) .orElse("UNKNOWN"); 值处理12Optional&lt;Soundcard&gt; soundcard = ...;soundcard.ifPresent(System.out::println); 123if(soundcard.isPresent())&#123; System.out.println(soundcard.get());&#125; 默认值1Soundcard soundcard = maybeSoundcard != null ? maybeSoundcard : new Soundcard("basic_sound_card"); 改进:1Soundcard soundcard = maybeSoundcard.orElse(new Soundcard("defaut")); 抛异常:12Soundcard soundcard = maybeSoundCard.orElseThrow(IllegalStateException::new); 过滤1234USB usb = ...;if(usb != null &amp;&amp; "3.0".equals(usb.getVersion()))&#123; System.out.println("ok");&#125; 改进:123Optional&lt;USB&gt; maybeUSB = ...;maybeUSB.filter(usb -&gt; "3.0".equals(usb.getVersion()) .ifPresent(() -&gt; System.out.println("ok")); StreamforEach12Random random = new Random();random.ints().limit(10).forEach(System.out::println); map123List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);//get list of unique squaresList&lt;Integer&gt; squaresList = numbers.stream().map( i -&gt; i*i).distinct().collect(Collectors.toList()); filter123List&lt;String&gt;strings = Arrays.asList("abc", "", "bc", "efg", "abcd","", "jkl");//get count of empty stringint count = strings.stream().filter(string -&gt; string.isEmpty()).count(); limit12Random random = new Random();random.ints().limit(10).forEach(System.out::println); sorted12Random random = new Random();random.ints().limit(10).sorted().forEach(System.out::println); Parallel Processing1234List&lt;String&gt; strings = Arrays.asList("abc", "", "bc", "efg", "abcd","", "jkl");//get count of empty stringlong count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count(); Collectors12345List&lt;String&gt; strings = Arrays.asList("abc", "", "bc", "efg", "abcd","", "jkl");List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList());System.out.println("Filtered List: " + filtered);String mergedString = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.joining(", ")); Statistics12345678List numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);IntSummaryStatistics stats = numbers.stream().mapToInt((x) -&gt; x).summaryStatistics();System.out.println("Highest number in List : " + stats.getMax());System.out.println("Lowest number in List : " + stats.getMin());System.out.println("Sum of all numbers : " + stats.getSum());System.out.println("Average of all numbers : " + stats.getAverage());]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis五种数据类型]]></title>
    <url>%2FRedis%E4%BA%94%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Redis五种数据类型的差异，适用场景等 不同数据结构的差异在于value的结构不一样 类型 对于Java 结构 应用 string(字符串) ArrayList 动态字符串提前扩容 存储序列化的对象 list(列表) LinkedList 链表 可做队列、栈；保存最新的10个用户，进来一个就删除一个，保持最新用户 hash(字段) HashMap 可以分别的存用户的信息 set(集合) HashSet value为null的HashMap 存中奖用户id，因为不可重复的特点 zset(有序集合) SortedSet和HashMap的结合 value有权重值的set 可以value为用户id，score为分数，按分数对用户排序操作(zrange 0 -1) string(字符串) 相当于Java中的ArrayList redis中的字符串是动态字符串，是可以修改的字符串 动态分配内存 字符串长度小于１MB时，扩容是加倍现有空间； 字符串大于１MB时，每次增加１MB; 字符串最大长度是512MB incr最大值是Long.Max 保存完整序列化对象list(列表) Java中的LinkedList 可做队列: 右push + 左pop 可做栈: 右push + 右pop zipList: 数量较少时，连续的地址空间存储，节约头尾指针的内存空间 应用: 保存最新的10个用户，进来一个新的就删除一个旧的；得到最新10个用户hash(字典) Java中的HashMap 单独存储用户信息的每个字段set(集合) 中奖用户id，不可重复Java中的HashSetzset(有序列表) value带有权重的set]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java面试]]></title>
    <url>%2FJava-interview%2F</url>
    <content type="text"><![CDATA[面试中高级的一些准备提纲, 持续更新 同学整理2019面经 整体参考: 高并发、分布式等，适合面试 基础知识+算法题解、源码分析 《Java编程的逻辑》系列博客 消息中间件 消息中间件面试常见问题 kafka架构和基本原理 kafka系统设计-面试 集合 Java集合总述 HashMap底层 HashMap源码 ConcurrentHashMap源码 LinkedHashMap TreeMap HashSet TreeSet ArrayList LinkedList 并发 线程的生命周期 synchronized 底层是通过monitor(监视器锁)对象完成的，通过软件在jvm实现 同步代码块是 monitorenter 和 monitorexit完成，同步方法是通过检查ACC_SYNCHRONIZED标识符是否被设置 实现方式是jvm在底层调用操作系统的互斥原语mutex实现，被阻塞的线程会被挂起、等待重新调度，会导致“用户态和内核态”两个态之间来回切换，对性能有较大影响。 其实wait/notify等方法也依赖于monitor对象 ThreadLocal源码分析 (重写) volatile原理 通过volatile修饰的变量可以保证线程之间的可见性，但并不能保证这多个指令的原子执行，在多线程并发执行下，无法做到线程安全，得到正确的结果 可见性:①写变量时，强制刷新到主内存中②修改变量后，强制让其他线程的工作内存中的值失效 顺序性:指令重排序，通过一组处理器指令，实现对内存操作的顺序限制(理解单例的懒加载的双重锁检查中的新建对象的三个过程) CAS / 参考2 比较，如果原始值没有被修改过，则更新成新的目标值；如果这一步的CAS没有成功，那就采用自旋的方式继续进行CAS操作，取出乍一看这也是两个步骤了啊，其实在 JNI 里是借助于一个 CPU指令 完成的。所以还是原子操作。 synchronized 是悲观锁，假设一定会发生冲突，直接加锁操作，比较重； CAS 是 乐观锁，假设不会发生冲突，当大声冲突时，进行重试操作； ReenterLock内部的AQS，还是各种Atomic开头的原子类，内部都应用到了CAS 缺点：ABA问题， 用AtomicStampedReference, 它可以通过控制变量值的版本来保证CAS的正确性。/ 循环时间长开销大 AQS即队列同步器。它是构建锁或者其他同步组件的基础框架(如ReentrantLock、ReentrantReadWriteLock、Semaphore等) 并发的几种实现方式 任务类实现Runnable接口，在方法Run()里定义任务。 任务类继承Thread，重写run()方法。 实现接口Callable并在call()方法里得到线程执行结果。 线程池比如coresize 1 maxsize 5 有界队列:先判断当前空闲线程数和coresize的关系，如果超过coresize扔队列，队列满，判断maxsize满没满，没满创建thread,如果满了执行拒绝策略逻辑(jdk自带４种拒绝策略，也可以重写，只要实现RejectExecutionHandler类) 线程池注意事项 有返回值的线程 多线程和并发基础面试题 java concurrent包常用类小结 CountDownLatch原理和示例 数据库MySql mysql笔记 索引详解 redis github参考 redis笔记 Jvm JVM jvm调优1 jvm调优2 jvm调优3框架dubbo 与springcloud的区别 灰度当一个接口出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互不调用，实现灰度发布,步骤如下: 接口旧的实现定义version=”1.0.0”，接口新的实现version=”2.0.0” Consumer端定义version=”*” 这样定义Provider和Consumer后，新旧接口实现各承担50%的流量；利用dubbo该特性，还能完成不兼容版本迁移：在低压力时间段，先升级一半Provider为新版本； 再将所有消费者升级为新版本； 然后将剩下的一半提供者升级为新版本。 dubbo是如何控制并发数和限流的？ Dubbo令牌桶限流代码分析-StatItem类 dubbo服务降级 dubbo sentinel dubbo spi 源码 掘金-肥朝 微服务架构基础之service meshspring spring ioc aop spring扩展点 分布式 分布式事务 分布式锁 redisSET resource_name my_random_value NX PX 30000NX：表示只有 key 不存在的时候才会设置成功。（如果此时 redis 中存在这个 key，那么设置失败，返回 nil）;PX 30000：意思是 30s 后锁自动释放。别人创建的时候如果发现已经有了就不能加锁了。优化: RedLock 算法; 对于redis cluster，尝试在大多数节点上建立锁就算成功，删除的时候依次删除 zookeeper尝试创建临时znode节点，创建成功就获取到了这个锁；那么其他机器再去尝试创建znode节点会失败，只能注册个监听器监听这个锁。释放锁就是删除这个znode，一旦释放就会通知客户端，然后一个等待的客户端可以再次重新加锁 redis需要不断尝试去获取锁，开销较大；zk获取不到锁时，注册个监听器即可 zookeeper 分布式协调Ａ系统发出请求以后可以在zookeeper上对某个节点的值注册个监听器,一旦Ｂ系统处理完成就修改zookeeper那个节点的值，Ａ系统立马可以收到通知 分布式锁创建znode节点，那么其他机器再去尝试创建znode节点会失败 元数据/配置信息管理 HＡ高可用一个重要的进程一般会做准备两个，主进程挂了立马通过zookeeper感知到切换刀备用进程 一致性hash zookeeper选举 算法 几大排序：描述和代码实现 参考 面试中的算法总结 快排实现: 分治 排序稳定性分析 二分查找 斐波那契 B树、B+树与红黑树 深度和广度优先 最长回文子串 懒加载单例 手写工厂模式 手写生产者消费者 动态代理 对称和非对称加密 刷题 剑指offer leetcode 其他 看看知识和岗位 简历模板 写简历 扫码登录]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM]]></title>
    <url>%2FJVM%2F</url>
    <content type="text"><![CDATA[Java内存模型，垃圾收集器、类加载 jdk包含jre，jre包含jvm 初试,分析解决查看堆存储快照VM arguments1-XX:+HeapDumpOnOutOfMemoryError 设置堆内存大小1-Xms20m -Xmx20m 分析工具：eclipse memory analyzer JVM监控工具jdk/bin/jconsole 直接运行堆内存，分为新生代和老年代。垃圾回收会对新生代进行回收。新生代分为，Eden和两个Survivor。垃圾回收会回收一部分Eden，存活的进入到Survivor生存区。存活久的进入到老年代。 Java虚拟机Sun Classic VM 第一款商用Java虚拟机 只能使用解释器方式执行java Exact VM 编译器和解释器混合执行，两级即时编译器 只在Solaris平台发布，就被取代了 HotSpot VM 称霸武林 非sun开发，后收购的 KVM 手机平台运行 JRockit BEA公司 专注服务器端 J9-IBM Microsoft JVMJava虚拟机内存结构分为： 线程独占区生命周期随着线程的创建而创建，随着线程的结束而死亡；所以不用关心这部分的垃圾回收。 程序计数器：当前线程执行字节码行号指示器。 虚拟机栈：一个个栈帧组成的，局部变量表、操作数栈、动态链接、方法出口信息；每个方法执行，创建一个栈帧，伴随方法的创建到结束，存储局部变量表等，方法执行完毕，出栈。 本地方法栈：与虚拟机栈类似，不过提供的是native方法运行。 线程共享区 堆：存储几乎所有的对象实例，虚拟机创建而创建。垃圾回收的主要区域。 方法区：存储类信息，常量，静态变量。编译后的代码。 程序计数器 一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。 处于线程独占区，线程私有 此区域是Java虚拟机规范中，唯一没有规定任何OutOfMemoryError情况的区域。 生命周期随着线程的创建而创建，随着线程的结束而死亡。 虚拟机栈 Java方法执行的动态内存模型 栈放的是对象的引用 栈帧：每个方法的执行，都创建一个栈帧，伴随方法的创建到执行完成，用于存储局部变量表，操作数栈，动态链接，方法出口。 当方法在运行过程中需要创建局部变量时，就将局部变量的值存入栈帧的局部变量表中。当这个方法执行完毕后，这个方法所对应的栈帧将会出栈，并释放内存空间。 栈内存溢出则，报错StackOverFlow（如递归） 注意：人们常说，Java的内存空间分为“栈”和“堆”，栈中存放局部变量，堆中存放对象。这句话不完全正确！这里的“堆”可以这么理解，但这里的“栈”只代表了Java虚拟机栈中的局部变量表部分。真正的Java虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。 本地方法栈 虚拟机栈为虚拟机执行Java方法服务，本地方法栈为虚拟机执行native方法服务。 本地方法栈和Java虚拟机栈实现的功能类似，只不过本地方法区是本地方法运行的内存模型。 堆 存储对象实例，(几乎所有的对象都存储在堆中) 虚拟机启动时创建 垃圾收集器管理的主要区域 老年代，新生代；新生代又被分为Eden、From Survivor、To Survivor。不同区域生命周期不同，根据区域垃圾回收。 -Xms -Xmx 方法区 存储虚拟机加载的类信息，常量，静态变量，即时编译器编译后的代码等数据。 方法区和永久代 垃圾回收在方法区的行为 异常的定义 运行时常量池，字符串会放到常量池里，类似hashset的table，如 String a = “aa”. String 的intern是把对象搬到运行时常量。 直接内存(不属于虚拟机内存) 除Java虚拟机以外的内存，也有可能被Java使用。 NIO， 在NIO中引入了一种基于通道和缓冲的IO方式。它可以通过调用本地方法直接分配Java虚拟机之外的内存，然后通过一个存储在Java堆中的DirectByteBuffer对象直接操作该内存，而无需先将外面内存中的数据复制到堆中再操作，从而提升了数据操作的效率。 直接内存大小不受Java虚拟机控制，不过，内存不足时就会抛出OOM异常。 综上 Java虚拟机的内存模型中一共有两个“栈”，分别是：Java虚拟机栈和本地方法栈。两个“栈”的功能类似，都是方法运行过程的内存模型。并且两个“栈”内部构造相同，都是线程私有。只不过Java虚拟机栈描述的是Java方法运行过程的内存模型，而本地方法栈是描述Java本地方法运行过程的内存模型。 Java虚拟机的内存模型中一共有两个“堆”，一个是原本的堆，一个是方法区。方法区本质上是属于堆的一个逻辑部分。堆中存放对象，方法区中存放类信息、常量、静态变量、即时编译器编译的代码。 对象的创建1、new 类名2、根据new的参数在常量池中定义一个类的引用符号。3、如果没有找到这个符号引用，说明类还没有被加载，进行类的加载，解析和初始化。4、虚拟机为对象分配内存，（堆）5、将分配的内存初始化为零值（不包括对象头）6、调用对象的init方法。（代码块，构造方法） 给对象分配内存 指针碰撞(指针移动)，连续的空间。 空闲列表：一张表，记录空闲内存。在空闲的部分分配内存。 线程安全 加锁 每个线程单独分配自己的区域，叫做本地线程分配缓冲 对象的结构 Header 自身运行时数据(Mark Word) 哈希值 GC分代年龄 锁状态标志 线程持有的锁 偏向线程ID 偏向时间戳 类型指针 InstanceData Padding 对象的访问定位栈的对象引用指向堆的对象实例。 使用句柄：指向堆中的句柄池，句柄池指向对象的地址 直接指针：直接指向对象的地址(hotspot) 具体哪一种方式，跟虚拟机有关。 垃圾回收如何判定对象为垃圾 引用计数法：在对象中添加一个引用计数器，当有地方引用这个对象的时候，引用计数+1，当引用失效引用计数-1.引用计数为0时回收。 可达性分析:根节点(GC Roots)不能到达的回收。GC Roots包括： 虚拟机栈 方法区的类属性所引用的对象 方法区中常量所引用的对象 本地方法栈中所引用的对象如何回收回收策略新生代通常复制算法，老年代标记-整理，标记清除。老年代对象一般较大不适合复制算法，浪费空间。标记-整理在可用对象多的情况下很适合。 标记-清除算法 效率问题 空间问题 复制算法:(针对新生代)解决标记清除的效率问题；两块相同的内存，每次只用一块区域，回收时把不需要回收的部分复制到另一块内存，并连续排列，原来的内存区全部清空，下一次在这块区域继续划分，循环这种操作。 内存浪费的解决方案。新生代分为Eden，From Survivor，To Survivor，比例是8：1：1。我们认为垃圾回收后剩余大约10%的有用对象。在Eden的对象垃圾收集以后剩余的放在Survivor，From与To之间是复制算法的关系。当Survivor内存不够时就需要一个内存担保，Tenured Gen。放到老年代。 标记-整理算法:针对老年代，整理以后清除。 分代收集算法：根据垃圾需要收集的多少决定用复制还是标记-整理算法 常见回收器(Java虚拟机没有规范，下面的不同收集器适用不同的场景) Serial(-XX:+UseSerialGC,新生代, 复制算法) 最基本，发展最悠久的 单线程(多线程任务执行，垃圾收集时全部线程暂停等待单线程的收集，收集完继续多个线程的执行) 桌面应用 ParNew(-XX:+UseParNewGC, 新生代, 复制算法) 多线程 Parallel Scavenge(-XX:+UserParellelGC) 复制算法(新生代收集器) 多线程收集器 吞吐量:(执行用户代码的时间) / 总时间 达到可控制的吞吐量 -XX：MaxGCPauseMillis 垃圾收集器最大停顿时间 -XX:GCTimeRatio 吞吐量大小， 取值(0, 100) Serial Old(-XX:+UseSerialOldGC, 标记-整理) 老年代收集器, Serial的老年代版本 Parallel Old 老年代 CMS(Concurrent Mark Sweep) 针对老年代 停顿时间很短 G1(-XX:+UseG1GC, 复制+标记+整理) 面向服务端 综合前面的优势 并行和并发 分代收集 空间整合 可预测的停顿 标记整理 何时回收内存分配策略1. 优先分配到eden打印垃圾收集信息： -verbose:gc -XX:+PrintGCDetails 2. 大对象直接分配到老年代指定大对象的大小： -XX:PretenureSizeThreshold=8M新生代通常是复制算法，新生代垃圾回收的频率很高，所以放在老年代更好。 3. 长期存活的对象分配到老年代(jdk7以后不严格)指定年龄多少算长期 -XX:MaxTenuringThreshold 15年龄计数器，每次垃圾回收年龄+1 4. 空间分配担保内存不够时借用老年代内存需要检查老年代是否有足够的空间容纳全部的新生代开启空间分配担保 -XX:+HandlePromotionFailure禁用空间分配担保 -XX:-HandlePromotionFailure 5. 动态对象的年龄判断6. 逃逸分析和栈上分配逃逸分析：分析对象的作用域。栈上分配是java虚拟机提供的一种优化技术 只在方法体内部有效，不发生逃逸。反之。为成员属性赋值，发生逃逸；对象的作用域仅在当前方法有效，没有发生逃逸。引用成员变量的值，发生逃逸。 没有发生逃逸的对象放在栈内存中。12345678910111213141516public class PartionOnStack &#123; static class User&#123; private int id; private String name; public User()&#123;&#125; &#125; private static User user; public static void foo() &#123; user=new User(); user.id=1; user.name="sixtrees"; &#125; public static void main(String[] args) &#123; foo(); &#125;&#125; 因为上面的代码中的User的作用域是整个Main Class，所以user对象是可以逃逸出函数体的。下面的代码展示的则是一个不能逃逸的代码段。1234567891011121314151617public class PartionOnStack &#123; class User&#123; private int id; private String name; public User()&#123;&#125; &#125; public void foo() &#123; User user=new User(); user.id=1; user.name="sixtrees"; &#125; public static void main(String[] args) &#123; PartionOnStack pos=new PartionOnStack(); pos.foo(); &#125;&#125; 虚拟机工具 Jps Java process status 控制台 打 “jps”. 显示Java进程id jps -l 运行时主类全名或jar包名 jps -m 运行时主类接收的参数(在args中) jps -v 接收的VM的参数 本地虚拟机唯一Id lvmid local virtual machine id Jstat 依赖jps， 需要知道进程的id才能使用 官网可查看文档 jstat -gcutil {id} 垃圾回收概要信息 Jinfo 实时查看和调整虚拟机的各项参数 jinfo -flag UseSerialGC {id} 查看该进程是否使用SerialGC。是(+),否(-) Jmap jmap -dump:format=b,file=/Users/yuan/a.bin {进程id} 转储快照 jmap -histo {id} 类和实例信息 Jhat 分析Jmap快照 jhat {文件路径} 启动http server Jstack 线程快照 Jstack {id} JConsole 内存监控 线程监控 死锁检测 VisualVM 需下载 性能调优案例1：绩效考核系统环境：64G， 2个intel E5 CPUtomcat7， jdk7堆内存设置了50G 问题：经常卡顿处理思路： 优化sql(不是每个功能慢了，是某个时间段慢了) 监控CPU 监控内存 Full GC 20-30s 解决：部署多个web容器，每个web容器堆内存设置为4G.用nginx负载。 这样解决了堆内存多大垃圾收集时间长。又不浪费内存。 总结：没有大对象，不经常Full GC，部署多个容器更好。否则单个容器分配大的内存更好。毕竟容器启动会有额外内存，硬盘开销。 案例2：数据抓取系统环境jdk5， 2G内存， intel core i3， win server 问题不定期内存溢出，堆内存加大也无济于事，导出堆快照，没人信息。内存监控，正常 处理思路捕获到了bytebuffer。系统用了很多NIO。direct memory改大一些。 案例3：物联网应用JVM崩溃。 Connect Reset 任务挤压，大量未处理任务导致解决：加消息队列 Class文件编译器编译产生的。字节码文件.class文件需要二进制编辑器打开。比如binary viewer 文件结构Class文件是一组以8位字节为基础单位的二进制流，各个数据项目严格按照顺序紧凑的排列。中间没有任何分隔符。Class文件中有两种数据类型：无符号数和表。 第一行前8位是魔数，后8位是版本号。魔数: class文件的标识，CA FE BA BE， 魔数后面8位为版本号：JDK1.8 = 52， JDK1.7 = 51 常量池访问标志类索引，弗雷索引，接口索引集合字段表集合方法表集合属性表集合 类加载自定义类加载123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class MyClassLoader extends ClassLoader &#123; private String path; private String name; public MyClassLoader(String path, String name) &#123; this.path = path; this.name = name; &#125; @Override public Class findClass(String name) &#123; byte[] b = loadClassData(name); return defineClass(name, b, 0, b.length); &#125; // 加载类文件二进制字节码 private byte[] loadClassData(String name) &#123; name = path + name + &quot;.class&quot;; InputStream in = null; ByteArrayOutputStream out = null; try &#123; in = new FileInputStream(new File(name)); out = new ByteArrayOutputStream(); int i = 0; while ((i = in.read()) != -1) &#123; out.write(i); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; out.close(); in.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; return out.toByteArray(); &#125;&#125;public class ClassLoaderCheck &#123; public static void main(String[] args) throws IllegalAccessException, InstantiationException &#123; MyClassLoader myClassLoader = new MyClassLoader(&quot;/Users/yuan/Desktop/&quot;, &quot;myClassLoader&quot;); Class clazz = myClassLoader.findClass(&quot;Bot&quot;); clazz.newInstance(); &#125;&#125; 类加载器分类: BootStrapClassLoader: C++编写，加载核心库java ExtClassLoader: Java编写，加载扩展库javax.* AppClassLoader: Java编写，加载程序所在目录 自定义ClassLoader: Java编写，定制化加载 双亲委派机制自底向上查找，自上向下加载;代码的loadclass中，加载类时，循环判断父加载器不为空就先尝试加载父加载器，直到加载C++编写的BootStrapClassLoader。 CustomClassLoader-&gt;AppClassLoader-&gt;ExtClassLoader-&gt;BooStrapClassLoader类本身+类加载器唯一确定在jvm 的唯一性。所以自定义加载同类名的类，无法加载参考 为什么用双亲委派模型?每个类加载器是一个类加载空间，隔开的各个类的加载优先级； 避免多份同样的字节码的加载，逐层去查找 loadClass, forName的区别 加载class文件字节码，生成Class对象 校验(正确和安全)、准备(为类变量分配空间并设置变量初始值)、解析(JVM常量池的符号引用转为直接引用) 初始化(执行类变量赋值和静态代码块) forName是加载类并已完成了初始化(比如执行静态代码块); loadClass没有链接的 三大性能调优参数 -Xms -Xmx -Xss的含义 -Xss: 规定每个线程虚拟机栈的大小，影响并发线程数的大小 -Xms: 堆的初始值 -Xmx: 堆能达到的最大值 一般-Xms与-Xmx设置成一样大小，防止扩容时的抖动 Java内存模型中堆和栈的区别-内存分配策略 静态存储: 编译时确定每个数据目标在运行时的存储空间需求 栈式存储: 数据区需求在编译时未知，运行时模块入口前确定 堆式存储: 编译时或运行时模块入口都无法确定，动态分配 管理方式: 栈自动释放，堆需要GC 空间大小: 栈比堆小 碎片相关: 栈产生的碎片远小于堆 分配方式: 栈支持静态和动态分配，堆只支持动态分配 效率: 栈效率比堆高 intern方法如果常量池中不存在，则把字符串放到常量池中，存在则直接返回其引用；JDK6以上:除了检查常量池是否存在，还检查堆中是否存在，如果存在则将对象的引用放在常量池并返回 Java内存结构线程私有: 程序计数器 虚拟机栈 本地方法栈 线程共享: 堆(JDK6以上: 常量池放在堆中，之前在永久代，频繁创建放到常量池容易OOM: PermGen spcae) MetaSpace: 类加载信息 GC Root 虚拟机栈中引用的对象(栈帧中的本地变量表) 方法区中的常量引用的对象 方法区中的类静态属性引用的对象 本地方法栈中JNI(Native方法)的引用对象 活跃线程的引用对象 回收算法 标记-清除 复制: 解决碎片化问题 标记-整理 分代收集 JDK8以后取消了永代，可以减少Full GC的频率 常见的垃圾收集器… 强引用、软引用、弱引用、虚引用 强引用(Strong Reference) Object obj = new Object() 抛出OOM也不会回收这个内存 软引用(Soft Reference) 对象处于有用但非必须的状态 只有空间不足时，才会回收这个对象内存 可以实现高速缓存12String str= new String(&quot;a&quot;)SoftReference&lt;String&gt; s = new SoftReference&lt;String&gt;(str) //软引用 弱引用(Weak Reference) 非必须对象，比软引用更弱 GC时被回收 被回收的概率不大，因为回收的优先级比较低 适用于偶尔会适用，不影响GC垃圾收集的对象 虚引用(Phntom Reference) 不会决定对象的生命周期 任何时候都能被回收 跟踪垃圾回收期回收活动，起到哨兵作用 必须和引用队列ReferenceQueue联合使用]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[logback kafka输出日志到ELK]]></title>
    <url>%2Flogback-kafka%E8%BE%93%E5%87%BA%E6%97%A5%E5%BF%97%E5%88%B0ELK%2F</url>
    <content type="text"><![CDATA[详细步骤，Spring日志通过logstash、kafka打印到ELK，方便查阅 参考 logback+kafka+elk搭建日志, 学习总结 日志流程: logback -&gt; kafka -&gt; logstash -&gt; elasticsearch -&gt; kibana kafka安装启动 官方下载， 选择Binary downloads下载 先启动zookeeperbin/zookeeper-server-start.sh config/zookeeper.properties &amp; 启动kafkabin/kafka-server-start.sh config/server.properties &amp;logback与kafka集成kafka与logback使用的是 logback-kafka-appender 引入pom依赖 123456789101112&lt;!--kafka依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.1.6.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!--logback-kafka-appender依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.danielwegener&lt;/groupId&gt; &lt;artifactId&gt;logback-kafka-appender&lt;/artifactId&gt; &lt;version&gt;0.2.0-RC2&lt;/version&gt;&lt;/dependency&gt; 配置logback-spring.xml: SpringBoot加载顺序: logback-spring.xml&gt; logback-spring.groovy&gt; logback.xml&gt; logback.groovy 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration scan="true" scanPeriod="60 seconds" debug="false"&gt; &lt;contextName&gt;logback&lt;/contextName&gt; &lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径--&gt; &lt;property name="LOG_HOME" value="/data/logs" /&gt; &lt;!--输出到控制台--&gt; &lt;appender name="console" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; %contextName [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender"&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;topic&gt;applog&lt;/topic&gt; &lt;!-- we don't care how the log messages will be partitioned --&gt; &lt;keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy" /&gt; &lt;!-- use async delivery. the application threads are not blocked by logging --&gt; &lt;deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" /&gt; &lt;!-- each &lt;producerConfig&gt; translates to regular kafka-client config (format: key=value) --&gt; &lt;!-- producer configs are documented here: https://kafka.apache.org/documentation.html#newproducerconfigs --&gt; &lt;!-- bootstrap.servers is the only mandatory producerConfig --&gt; &lt;producerConfig&gt;bootstrap.servers=localhost:9092&lt;/producerConfig&gt; &lt;!-- don't wait for a broker to ack the reception of a batch. --&gt; &lt;producerConfig&gt;acks=0&lt;/producerConfig&gt; &lt;!-- wait up to 1000ms and collect log messages before sending them as a batch --&gt; &lt;producerConfig&gt;linger.ms=1000&lt;/producerConfig&gt; &lt;!-- even if the producer buffer runs full, do not block the application but start to drop messages --&gt; &lt;producerConfig&gt;max.block.ms=0&lt;/producerConfig&gt; &lt;!-- define a client-id that you use to identify yourself against the kafka broker --&gt; &lt;producerConfig&gt;client.id=$&#123;HOSTNAME&#125;-$&#123;CONTEXT_NAME&#125;-logback-relaxed&lt;/producerConfig&gt; &lt;/appender&gt; &lt;root level="info"&gt; &lt;appender-ref ref="console" /&gt; &lt;appender-ref ref="kafkaAppender" /&gt; &lt;/root&gt;&lt;/configuration&gt; logstash配置启动 ELK的安装使用可以参考 ELK安装使用 配置kafka接收数据，输出到es, index是 test-kafka12345678910111213input &#123; kafka &#123; topics =&gt; "applog" bootstrap_servers =&gt; "localhost:9092" group_id =&gt; "es" &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; "localhost:9200" index =&gt; "test-kafka" &#125;&#125; 启动./bin/logstash -f test-kafka.confelasticsearch启动./bin/elasticsearch kibana./bin/kibana 验证 启动程序输出日志1234567891011121314@Slf4j@SpringBootApplicationpublic class LogKafkaApplication &#123; public static void main(String[] args) throws InterruptedException &#123; SpringApplication.run(LogKafkaApplication.class, args); while (true) &#123; Thread.sleep(5000); log.info("log to kafka..."); &#125; &#125;&#125; 访问 http://127.0.0.1:5601, 在test-kafka下出现了日志,如图]]></content>
      <categories>
        <category>工具框架</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring 多数据源]]></title>
    <url>%2FSpring-%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%2F</url>
    <content type="text"><![CDATA[Spring多数据源的简单概述 application.properties1234567891011management.endpoints.web.exposure.include=*spring.output.ansi.enabled=alwaysfoo.datasource.url=jdbc:h2:mem:foofoo.datasource.username=safoo.datasource.password=bar.datasource.url=jdbc:h2:mem:barbar.datasource.username=sabar.datasource.password= Application.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Slf4j@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class, DataSourceTransactionManagerAutoConfiguration.class, JdbcTemplateAutoConfiguration.class&#125;)public class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125; @Bean @ConfigurationProperties("foo.datasource") public DataSourceProperties fooDataSourceProperties() &#123; return new DataSourceProperties(); &#125; @Bean public DataSource fooDataSource() &#123; DataSourceProperties dataSourceProperties = fooDataSourceProperties(); log.info("foo dataSource: [&#123;&#125;]", dataSourceProperties.getUrl()); return dataSourceProperties.initializeDataSourceBuilder().build(); &#125; @Bean @Resource public PlatformTransactionManager fooTxManager(DataSource fooDataSource) &#123; return new DataSourceTransactionManager(fooDataSource()); &#125; @Bean @ConfigurationProperties("bar.datasource") public DataSourceProperties barDataSourceProperties() &#123; return new DataSourceProperties(); &#125; @Bean public DataSource barDataSource() &#123; DataSourceProperties dataSourceProperties = barDataSourceProperties(); log.info("bar dataSource: [&#123;&#125;]", dataSourceProperties.getUrl()); return dataSourceProperties.initializeDataSourceBuilder().build(); &#125; @Bean @Resource public PlatformTransactionManager barTxManager(DataSource barDataSource) &#123; return new DataSourceTransactionManager(fooDataSource()); &#125;&#125; 忽略springboot自动配置的数据源部分，自己配置]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[我的 Intelij Idea 配置]]></title>
    <url>%2F%E6%88%91%E7%9A%84-Intelij-Idea-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[我常用的Idea字体，插件等配置，欢迎查阅 代码模板 在 Preference/Editor/File and Code Templates Class12345678910#if ($&#123;PACKAGE_NAME&#125; &amp;&amp; $&#123;PACKAGE_NAME&#125; != &quot;&quot;)package $&#123;PACKAGE_NAME&#125;;#end#set($myName = &apos;xxx&apos;)/** * $&#123;DESCRIPTION&#125; * @author $&#123;myName&#125; * @date $&#123;YEAR&#125;-$&#123;MONTH&#125;-$&#123;DAY&#125; $&#123;TIME&#125; */public class $&#123;NAME&#125; &#123;&#125; Interface 1234567891011#if ($&#123;PACKAGE_NAME&#125; &amp;&amp; $&#123;PACKAGE_NAME&#125; != &quot;&quot;)package $&#123;PACKAGE_NAME&#125;;#end#set($myName = &apos;xxx&apos;)/** * $&#123;DESCRIPTION&#125; * @author $&#123;myName&#125; * @date $&#123;YEAR&#125;-$&#123;MONTH&#125;-$&#123;DAY&#125; $&#123;TIME&#125; */#parse(&quot;File Header.java&quot;)public interface $&#123;NAME&#125; &#123;&#125; Enum 1234567891011#if ($&#123;PACKAGE_NAME&#125; &amp;&amp; $&#123;PACKAGE_NAME&#125; != &quot;&quot;)package $&#123;PACKAGE_NAME&#125;;#end#set($myName = &apos;xxx&apos;)/** * $&#123;DESCRIPTION&#125; * @author $&#123;myName&#125; * @date $&#123;YEAR&#125;-$&#123;MONTH&#125;-$&#123;DAY&#125; $&#123;TIME&#125; */#parse(&quot;File Header.java&quot;)public enum $&#123;NAME&#125; &#123;&#125; 字体 在 Preference/Editor/Font 很喜欢mac字体Menlo，windows可自行下载字体安装，逐个点击字体文件安装即可 插件 Lombok : 简化Java代码，自动生产Setter、Getter等，详情见官网 Free Mybatis :可以将Mapper.java与xml文件关联起来，点击可以跳转到对应的方法中 Maven Helper :maven解决冲突，查看maven引用tree Alibaba Java Coding Guidelines : 阿里代码规约，根据阿里代码规约自动提示]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【kong二】kong的简单使用-konga]]></title>
    <url>%2F%E3%80%90kong%E4%BA%8C%E3%80%91kong%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8-konga%2F</url>
    <content type="text"><![CDATA[通过konga简单的配置kong，的使用过程如图， Upstream Url: 重定向的地址是实际的接口地址，或者upstreams地址(此时upstream配合多台机器接口，可做负载均衡) Hosts:kong主机ip Uris：访问地址 此时我们访问http://{hosts}:8000/Uris，则指向了Upstream URL 负载 1.配置UPSTREAMS新建一个UPSTREAMS，在其下增加多个Targets：Target配置为ip:port,修改Weight权重 2.配置APIS新建APIS，配置填写如下123456&#123; &quot;name&quot;:&quot;test&quot;, &quot;host&quot;:&quot;$&#123;kong所在主机ip&#125;&quot;, &quot;Uris&quot;:&quot;/api/test&quot;, &quot;Upstream URL&quot;:&quot;http://$&#123;UPSTREAMS.name&#125;/api/test&quot;,&#125; kong访问地址: http://${kong主机}:8000/${Uris}]]></content>
      <categories>
        <category>工具框架</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【kong一】kong安装与konga]]></title>
    <url>%2F%E3%80%90kong%E4%B8%80%E3%80%91kong%E5%AE%89%E8%A3%85%E4%B8%8Ekonga%2F</url>
    <content type="text"><![CDATA[kong的详细安装过程，以及通过konga管理台管理环境 ubuntu 一、安装PostgreSqlkong需要用到数据库，目前支持psql安装过程参考安装完成创建一个用户:kong，密码:kong，数据库:kong 二、安装Kong1. 文件下载2. 安装123sudo apt-get updatesudo apt-get install openssl libpcre3 procps perlsudo dpkg -i kong-community-edition-0.11.0.*.deb 3. 修改数据库配置文件12cp /etc/kong/kong.conf.default /etc/kong/kong.confvim /etc/kong/kong.conf 示例：12345pg_host = 127.0.0.1 # The PostgreSQL host to connect to.pg_port = 5432 # The port to connect to.pg_user = kong # The username to authenticate if required.pg_password = kong # The password to authenticate if required.pg_database = kong 4. 启动12kong migrations upkong start 5. 验证1curl http://127.0.0.1:8001 输出大段json信息说明成功 6. 端口说明8000端口：外部调用api端口，比如设置Urlis=/test, 则访问地址是: http://[IP]:8000/test8001端口: api 管理端口，可通过restful接口管理kong 7. 文档https://docs.konghq.com/ 三、安装Konga说明： 官网推荐: kong-dashboard，但对比界面高端程度和友好度，更推荐konga. [一个坑]kong版本问题：我在安装时目前kong最新版本已经到1.0.0, 对于konga和kong-dashboard还不支持，建议安装低版本0.15以下; 具体表现查询apis，旧版本:[ip]:8001/apis,新版:[ip]:8001/services npm安装方式1. 安装依赖1234sudo apt-get install nodejs npmsudo npm install -g gulpsudo npm install -g bowersudo npm install -g sails 2. 安装123git clone https://github.com/pantsel/konga.gitcd konganpm install konga 3. 配置数据库信息目前支持数据库: mysql, mongo, sqlserver, postgres1234567891011cd config/cp local_example.js local.jsvi local.jsmodels: &#123; connection: process.env.DB_ADAPTER || 'localDiskDb',&#125;# 改成models: &#123; connection: process.env.DB_ADAPTER || 'mysql', // 这里可以用‘mysql’，‘mongo’，‘sqlserver’，‘postgres’&#125; 12345678910111213141516171819# 修改数据库默认配置vi konga/config/connections.jsmysql: &#123; adapter: 'sails-mysql', host: process.env.DB_HOST || 'localhost', port: process.env.DB_PORT || 3306, user: process.env.DB_USER || 'root', password: process.env.DB_PASSWORD || null, database: process.env.DB_DATABASE || 'konga_database'&#125;# 改成mysql: &#123; adapter: 'sails-mysql', host: process.env.DB_HOST || 'localhost', port: process.env.DB_PORT || 3306, user: process.env.DB_USER || 'root', password: process.env.DB_PASSWORD || 'root', database: process.env.DB_DATABASE || 'konga_database'&#125; 安装依赖123npm run bower-depsnpm install dotenv-extendednpm install angular 启动12#konga根目录npm start 访问: http://127.0.0.1:1338]]></content>
      <categories>
        <category>工具框架</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[openresty mac 安装与使用]]></title>
    <url>%2Fopenresty-mac-%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[openresty详细安装步骤 安装 依赖安装 12brew updatebrew install pcre openssl 安装1brew install homebrew/nginx/openresty 设置环境变量 12PATH=/usr/local/opt/openresty/nginx/sbin:$PATHexport PATH 3.启动查看查看版本信息1nginx -V 启动1nginx 常用命令1234启动:nginx停止:nginx -s stop 停止nginx也停止了openresty重启:nginx -s reload检验nginx配置是否正确: nginx -t Hello World 创建测试目录 1mkdir ~/openresty-test ~/openresty-test/logs/ ~/openresty-test/conf/ 在conf文件下创建nginx.conf文件 12345678910111213141516worker_processes 1;error_log logs/error.log;events &#123; worker_connections 1024;&#125;http &#123; server &#123; listen 8088; location / &#123; default_type text/html; content_by_lua ' ngx.say("hello lua") '; &#125; &#125;&#125; nginx -p ${pwd} -c conf/nginx.conf 访问 http://localhost:8888]]></content>
      <categories>
        <category>openresty</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo更新博客-原始文件与静态文件]]></title>
    <url>%2FHexo%E6%9B%B4%E6%96%B0%E5%8D%9A%E5%AE%A2-%E5%8E%9F%E5%A7%8B%E6%96%87%E4%BB%B6%E4%B8%8E%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言通过Hexo发布的博客，通过本地原始文件 “hexo g -d” 将生成的静态文件发布到github上，原始文件留在了本地。这样无法在其他电脑或文件丢失情况下更新发布博客。 解决利用github的分支，创建两个分支master和hexo； hexo分支存储原始文件，master存储博客静态页面，原始文件push到hexo分支，而静态文件直接发布到了master分支。_config.yml中的deploy参数，分支应为master。 日常博客修改包括新博文和样式的修改 1.提交原始文件到分支hexo123git add .git commit -am &quot;&quot;git commit origin hexo 2.发布网站到master分支上1hexo g -d 注意顺序 在没有原始文件的电脑上修改 拉hexo分支的原始代码到本地 1git clone ... 下载npm包(需要有node环境) 123npm install hexonpm installnpm install hexo-deployer-git 不需要 hexo init]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK安装使用]]></title>
    <url>%2FELK%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[ELK安装详细步骤，欢迎参考 logstash 需要java环境 官方下载地址cd [logstash主目录]vim test.conf12345678input &#123; stdin &#123; &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#123;&#125; &#125;&#125; 运行: bin/logstash -f test.conf测试结果上面结果表示成功 elasticsearch官方下载地址cd [elasticsearch主目录]vim config/elasticsearch.yml1234path.data: /data/es #数据路径path.logs: /data/logs/es #日志路径network.host: 本机地址 #服务器地址http.port: 9200 #端口 启动: bin/elasticsearch 集成logstash和elasticsearch修改logstash配置文件123456789101112input &#123; stdin &#123; &#125; &#125;output &#123; elasticsearch &#123; hosts =&gt; &quot;elasticsearchIP:9200&quot; index =&gt; &quot;logstash-test&quot; &#125; stdout &#123; codec =&gt; rubydebug &#123;&#125; &#125; &#125; 再次启动logstash, 输入”hello elasticsearch” kibana官方下载地址cd [kibana主目录]vim config/kibana.yml123server.port: 5601 #kibana端口server.host: “本机ip” #kibana的ipelasticsearch.url: “http://elasticsearchIP:9200” #elasticsearch地址 SpringBoot-ELK demo: 推荐]]></content>
      <categories>
        <category>工具框架</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java集合总述]]></title>
    <url>%2FJava-Collection%2F</url>
    <content type="text"><![CDATA[整体总结概述各集合特点，底层数据结构 结构 Collection List * ArrayList:Object数组实现，每次扩容为原来的1.5倍；非线程安全； Vector :Object数组实现，相对ArrayList，用synchronized关键字加锁，实现线程安全； * LinkedList:双向循环链表；非线程安全； Map * HashMap:存储&lt;K,V&gt;对象，用K计算的hash值匹配到bucket，K.equals()定位具体的&lt;K,V&gt;对象来找到value；负载因子0.75，也就是说容量达到75%时扩容，初始16，每次扩容变为原来的2倍。 Hashtable:遗留类，基本被淘汰，放入null会报错。它是在HashMap基础加了synchronized关键字，所以并发性不好；替代的推荐用ConcurrentHashMap * LinkedHashMap:HashMap的一个子类；链表实现 * TreeMap：TreeMap实现SortedMap接口，保存的是按key排序的，默认是按key升序排序，也可以指定排序；红黑树 * ConcurrentHashMap:JDK1.8 中取消了Segment分段锁，采用CAS和synchronized来保证并发安全。数据结构跟HashMap1.8的结构类似，数组+链表/红黑二叉树。不允许键值null； Set * HashSet:无序，唯一；基于 HashMap 实现的，底层采用 HashMap 来保存元素；判断对象重复的方式与HashMap判断K的hash然后equals一致； * TreeSet：有序，唯一；红黑树。基于TreeSet LinkedHashSet：基于LinkedHashMap实现 推荐: 源码学习 HashMap分析 允许键值为null； 非线程安全；如果需要满足线程安全，可以用 Collections.synchronizedMap()方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 JDK1.7前，以数组+链表的链表数组存储，数组是主体，链表是为了解决hash冲突的存在。存储的结构是&lt;K,V&gt;的实体，以K的hash值定位到bucket，发生hash冲突是通过K.equals()找到需要的实体部分。 JDK1.8，当链表长度&gt;8时，链表转换为红黑树，当链表长度&lt;6，红黑树转为链表。另外改进了hash计算方法，使得分布更加均匀(右移16位相异或，使得高位和低位的特征都存在，hash碰撞率低) 负载因子过大，则链表过长；负载因子过小，则hash桶过多，空间复杂度过高 为什么HashMap长度是2的N次？12345这个算法实际就是取模，hash%length，计算机中直接求余效率不如位移运算，源码中做了优化hash&amp;(length-1)，hash%length==hash&amp;(length-1)的前提是length是2的n次方；为什么这样能均匀分布减少碰撞呢？2的n次方实际就是1后面n个0，2的n次方-1 实际就是n个1；例如长度为9时候，3&amp;(9-1)=0 2&amp;(9-1)=0 ，都在0上，碰撞了；例如长度为8时候，3&amp;(8-1)=3 2&amp;(8-1)=2 ，不同位置上，不碰撞； HashMap put方法逻辑 如果未初始化过，则初始化 对key求hash，然后计算下标 如果没有碰撞，直接放入桶中 如果碰撞了，已链表方式链接到后面 如果链表长度超过阈值，链表转为红黑树 如果节点已存在就替换旧值 如果桶满了就resize(扩容2倍后重排) 为什么8节点变为红黑树？ n/2与log(n) 概率统计得到 ConcurrentHashMapConcurrentHashMap size方法 ConcurrentHashMap put过程 key和value任意为空，抛出异常 计算key的hash值，遍历table数组 table为null或空，调用initTable初始化 为保证可见性，table数组取数据，CAS保证可见性和线程安全性 若当前桶位hash值MOVED，说明正在扩容，则帮助迁移 元素不止一个，synchronized给头节点加锁，链表或红黑树的插入节点]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java编程的逻辑-泛型]]></title>
    <url>%2FJava%E7%BC%96%E7%A8%8B%E7%9A%84%E9%80%BB%E8%BE%91-%E6%B3%9B%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[泛型是通过类型擦除实现的，类型参数会在编译时被替换为Object，运行时Java虚拟机不知道泛型这回事；类泛型，方法泛型，接口泛型 泛型类1234567891011121314public class Pair&lt;U,V&gt; &#123; U first; v second; public Pair(U first, V second) &#123; this.first = first; this.second = second; &#125; public U getFirst() &#123; return first; &#125; public V getSecond() &#123; return second; &#125; 泛型方法123456789101112131415161718192021222324252627282930313233343536//下面都是泛型方法public &lt;T&gt; int indexOf(T[] arr, T elm) &#123; for(int i=0; i &lt; arr.length; i++) &#123; if(arr[i].equals(elm)) &#123; return i; &#125; &#125; return -1;&#125;public &lt;T,K&gt; K showKeyName(Generic&lt;T&gt; container)&#123;...&#125;//这不是一个泛型方法，这是一个普通的方法，只不过使用了泛型通配符? //同时这也印证了泛型通配符章节所描述的，?是一种类型实参，可以看做为Number等所有类的父类 public void showKeyValue2(Generic&lt;?&gt; obj)&#123; Log.d("泛型测试","key value is " + obj.getKey()); &#125; class GenerateTest&lt;T&gt;&#123; public void show_1(T t)&#123; System.out.println(t.toString()); &#125; //在泛型类中声明了一个泛型方法，使用泛型E，这种泛型E可以为任意类型。可以类型与T相同，也可以不同。 //由于泛型方法在声明的时候会声明泛型&lt;E&gt;，因此即使在泛型类中并未声明泛型，编译器也能够正确识别泛型方法中识别的泛型。 public &lt;E&gt; void show_3(E t)&#123; System.out.println(t.toString()); &#125; //在泛型类中声明了一个泛型方法，使用泛型T，注意这个T是一种全新的类型，可以与泛型类中声明的T不是同一种类型。 public &lt;T&gt; void show_2(T t)&#123; System.out.println(t.toString()); &#125; &#125; 表明该方法将使用泛型类型T，此时才可以在方法中使用泛型类型T; 只有声明了的方法才是泛型方法，泛型类中的使用了泛型的成员方法并不是泛型方法。 泛型方法与可变参数12345public &lt;T&gt; void printMsg( T... args)&#123; for(T t : args)&#123; Log.d("泛型测试","t is " + t); &#125;&#125; 1printMsg("111",222,"aaaa","2323.4",55.55); 泛型方法静态方法 静态方法无法访问类上定义的泛型；如果静态方法操作的引用数据类型不确定的时候，必须要将泛型定义在方法上。 泛型接口1234//定义一个泛型接口public interface Generator&lt;T&gt; &#123; public T next();&#125; 1234567891011/** * 未传入泛型实参时，与泛型类的定义相同，在声明类的时候，需将泛型的声明也一起加到类中 * 即：class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;&#123; * 如果不声明泛型，如：class FruitGenerator implements Generator&lt;T&gt;，编译器会报错："Unknown class" */class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;&#123; @Override public T next() &#123; return null; &#125;&#125; 1234567891011//实现时传入具体类型public class FruitGenerator implements Generator&lt;String&gt; &#123; private String[] fruits = new String[]&#123;"Apple", "Banana", "Pear"&#125;; @Override public String next() &#123; Random rand = new Random(); return fruits[rand.nextInt(3)]; &#125;&#125; 泛型通配符举例， 虽然Integer是Number的子类，但是Generic和Generic并不能构成子类关系；可以使用下面的形式123public void showKeyValue1(Generic&lt;?&gt; obj)&#123; Log.d("泛型测试","key value is " + obj.getKey());&#125; 此处的 ‘?’ 是实参，可以把 ‘?’ 看成所有类型的父类。是一种真实的类型; 当操作类型时，不需要使用类型的具体功能时，只使用Object类中的功能。那么可以用 ? 通配符来表未知类型。 两种写法:123public void addAll(DynamicArray&lt;? extends E&gt; c)public &lt;T extends E&gt; void addAll(DynamicArray&lt;T&gt; c) 通配符，允许读不允许写，比如下面，?是未知的类型，只知道是Number的子类，如果允许写，Java无法保证类型安全性所以干脆禁止写12345678910111213```#### 上界```javapublic class NumberPair&lt;U extends Number, V extends Number&gt; extends Pair&lt;U, V&gt; &#123; public NumberPair(U first, V second) &#123; super(first, second); &#125;&#125;public static &lt;T extends Comparable&lt;T&gt;&gt; T max(T[] arr) &#123; //&#125; 局限性 因为类型会被替换为Object，所以泛型不能使用基本的数据类型; 下面写法不合法，需使用对应的包装类 1Pair&lt;int&gt; minmax = new Pair&lt;ing&gt;(1,100) instanceof是运行时判断，与泛型无关 下面的例子重载不允许，因为类型擦除后它们的声明是一样的 12public static void test(DynamicArray&lt;Integer&gt; intArr)public static void test(DynamicArray&lt;String&gt; intArr) 不能用类型参数创建对象；如果允许用户以为创建的是对应类的对象，类型擦除后实际创建的是Object的对象，所以干脆禁止。 想要这么做可以用反射； 123//都是非法的T elm = new T();T[] arr = new T[10]; 不能用于静态变量 不支持创建泛型数据， Object[] -&gt; T[] 问题； 需要使用反射。Array.newInstance(Class, size)]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring RestTemplate 使用与HttpClient]]></title>
    <url>%2FSpring-RestTemplate-%E4%BD%BF%E7%94%A8%E4%B8%8EHttpClient%2F</url>
    <content type="text"><![CDATA[1.引入java请求网络资源通常用HttpClient等，Spring封装了库，提供更为简洁的资源请求方式RestTemplate。 2.简单GET请求(HttpClient/RestTemplate)HttpClient12345678910private static final CloseableHttpClient httpclient = HttpClients.createDefault();HttpGet httpget = new HttpGet(url);CloseableHttpResponse response = httpclient.execute(httpget);String result = null;HttpEntity entity = response.getEntity();if (entity != null) &#123; result = EntityUtils.toString(entity);&#125;System.out.println(result); RestTemplate1234//建议以注入方式使用RestTemplate restTemplate=new RestTemplate();String result = restTemplate.getForObject(url, String.class);System.out.println(result); 对比下，明显RestTemplate方式更加优雅简洁；并提供方法将返回值映射为指定对象。 3.Spring注入方式使用12345678910@Configuration public class RestClientConfig &#123; @Bean public RestTemplate restTemplate() &#123; RestTemplate restTemplate = new RestTemplate(); return restTemplate; &#125; &#125; 123456789@Service public class UserService &#123; @Autowired private RestTemplate restTemplate; // ... &#125; 4.RestTemplate的其他方法 getForObject: 发送get请求，结果封装为指定对象。 提供class指定 getForEntity: 发送get请求，结果为Entity类型。 postForObject: 发送post请求，结果封装为指定对象 put: delete: exchange:通用执行方法…exchange发送POST，APPLICATION_JSON示例12345HttpHeaders headers = new HttpHeaders();headers.setContentType(MediaType.APPLICATION_JSON);HttpEntity&lt;String&gt; entity = new HttpEntity&lt;&gt;(new Gson().toJson(requestData), headers);ResponseEntity&lt;String&gt; response = restTemplate .exchange(url, HttpMethod.POST, entity, String.class); 5.RestTemplate的请求301，302问题描述：使用过程发现，遇到301请求不能自动转发问题。 解决：想起了使用HttpClient过程，实现请求自动转发需要创建HttpClient时设置重定向策略。翻阅资料发现了这一段。 123456HttpComponentsClientHttpRequestFactory factory = new HttpComponentsClientHttpRequestFactory();HttpClient httpClient = HttpClientBuilder.create() .setRedirectStrategy(new LaxRedirectStrategy()) .build(); factory.setHttpClient(httpClient); restTemplate.setRequestFactory(factory); 于是问题解决。 分析：查阅源码可见，RestTemplate执行请求时。创建(createRequest)了一个HttpUriRequest来完成请求。createRequest具体在ClientHttpRequestFactory 实现，而上面代码中的HttpComponentsClientHttpRequestFactory，这个工厂在创建请求时正好用的apache下 httpClient的参数。由此可见。上面代码底层是以HttpClient方式发送的请求。 6.RestTemplate全部是以HttpClient方式请求的吗？通过源码不难看到，默认RestTemplate使用的是SimpleClientHttpRequestFactory工厂。追踪可见，默认它是以java.net下的HttpURLConnection方式发起的请求。所以RestTemplate是支持多种方式发起请求的。查看该工程的实现类可知，支持包括HttpClient, OkHttp等方式，如下图]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>RestTemplate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python入门-基础语法笔记]]></title>
    <url>%2Fpython-base-note%2F</url>
    <content type="text"><![CDATA[python基础语法，比较详细, 查阅和初学 1.vi技巧：中英切换：shiftwq = x 2.注释单行：#多行：三个单引号或三个双引号123"""print("hello world") """ 3.编码文件中有中文，不管是否为注释，python2执行报错。解决：程序首行加 #coding=utf-8 或 #*-* coding:utf-8 *-* 4.输入1name = input("请输入名字：") 5.输出123456789101112131415print("name is %s"%name) print("name:%s, age:%d"%(name,age)) print("name:&#123;0&#125;, age:&#123;1&#125;".format(name,age)) #(标号可不写，为默认顺序) print("name:&#123;name&#125;, age:&#123;age&#125;".format(name = "ss",age = 23)) import mathprint('常量 PI 的值近似为 &#123;0:.3f&#125;。'.format(math.pi)) #常量 PI 的值近似为 3.142。table = &#123;'Google': 1, 'Runoob': 2, 'Taobao': 3&#125;print('Runoob: &#123;0[Runoob]:d&#125;; Google: &#123;0[Google]:d&#125;; Taobao: &#123;0[Taobao]:d&#125;'.format(table))#Runoob: 2; Google: 1; Taobao: 3table = &#123;'Google': 1, 'Runoob': 2, 'Taobao': 3&#125;print('Runoob: &#123;Runoob:d&#125;; Google: &#123;Google:d&#125;; Taobao: &#123;Taobao:d&#125;'.format(**table))#Runoob: 2; Google: 1; Taobao: 3 6.类型转换1a = int("3") 7.python2与python3 编码。 python2中input的内容当做执行的内容，python3当做字符串。如a = input(“”). python2中的raw_input相当于python3中的input。 python2 中支持 &lt;&gt; 是不等于的意思，python3不支持，应该 != 表示 8.运算符12345"a" * 5 = "aaaaa"2 ** 3 = 85 / 2 = 2.55 // 2 = 2 #(取商)5 % 2 = 1 #（取余） 9.逻辑运算符and , or 10.流程123456789101112131415if 1 &gt; 2: print("1")elif 2 == 3: print("2")else: print("3") while 条件: 执行for i int str: 执行 注: i++,++i在python中不允许使用。 11.字符串(不可变) 格式化 123456789101112131415161718192021&gt;&gt;&gt;'a: %s, b:%s' %s ('a', 'b')'a: a, b: b'&gt;&gt;&gt; '&#123;&#125;, &#123;&#125; and &#123;&#125;'.format("first", "second", "third")'first, second and third'&gt;&gt;&gt; '&#123;3&#125; &#123;0&#125; &#123;2&#125; &#123;1&#125; &#123;3&#125; &#123;0&#125;'.format('be', 'not', 'or', 'to')'to be or not to be'&gt;&gt;&gt; "&#123;name&#125; is approximately &#123;value:.2f&#125;.".format(value=pi,name="π")'π is approximately 3.14.'#python3 中，同名可省略&gt;&gt;&gt; from math import e&gt;&gt;&gt; f"Euler's constant is roughly &#123;e&#125;."&gt;&gt;&gt; f"Euler's constant is roughly &#123;e&#125;.".format(e=e)#格式&gt;&gt;&gt; 'the number is &#123;num:f&#125;'.format(num=2)'the number is 2.000000' 类型转换： 1int("100"), str(100) 长度： 1len(str) 字符串连接: 123a = b + c #或者a = "===%s==="%(b+c) #或者a = "===&#123;&#125;===".format(b+c) 切片： 1234str = "dasfaf"str[2:4] #(取得2到3的), str[2:] #(到最后), str[2:-1:2] #(步长2，隔一个取一个) 逆序: 123456str = "abcdefABCDEF" str[0:] # out:"abcdefABCDEF" str[-1:] # out:"F" str[-1:0] # out:"" str[-1:0:-1] # out:"FEDCBAfedcb" str[-1::-1], str[::-1] # out:"FEDCBAfedcba" 常见操作 123456789101112131415161718192021222324find: str.find("abc") # 从左向右有返回第一个匹配字符串的起始下标，没有返回-1。rfind():从右向左。 str.index("abc") #找到返回起始下标，没有抛出异常。 存在rindex(). str.count("abc") #匹配的个数。str.replace("abc", "def") #把左边的替换成右边的同java， str.replace("abc", "def",1) #第三个参数是从左到右替换个数。 str.split(" ") #字符串分割 str.capitalize() #字符串的第一个字母大写。 str.title() #字符串的每个单词的首字母大写。 str.startswith("abc"), str.endswith("abc") #是否以此字符串开头或结尾. str.lower() str.uper() #所有的字母小写和大写。 str.center(50) #居中显示，行总长50 " abc " str.ljust(50), str.rjust(50) #左(右)对齐str.lstrip() #删除左边空格str.rstrip() #右边空格str.strip() #删除两端空格.#center居中&gt;&gt;&gt; 'hello world'.center(20)' hello world '&gt;&gt;&gt; 'hello world'.center(20, '*')'****hello world*****' 12.列表 (类似数组，可变，针对自身的变化)12345678910111213141516171819202122232425262728["zhangsan","lisi"]#定义names = ["zhangsan", "lisi", 3.14] #列表中的数据可以是不同的数据类型。 可以下标，和切片。#增names.append("abc") #--&gt;插入到最后; names.insert(0, "bcc") #--&gt;插入指定位置。 names = names1 + names2 #两个列表用连接符names1.extend(names2) #扩充#删names.pop() #--&gt;删除最后一个； names.remove("lisi") #--&gt;根据内容删除; del names[0] #--&gt;下标删除 #改names[0] = "abc" #查name[1:] # "lisi"in, not in #是否存在 （if "zhangsan" in names:） #可以for... in 循环遍历len(names) #元素个数#注意：append添加的元素；extend连接的列表# 其他''.join(list) # 列表转字符串names.sort() #排序，改变原结构; names.sort(reverse=True) 反向顺序 运算符 表达式 结果 描述 len([1, 2, 3]) 3 计算元素个数 [1, 2, 3] + [4, 5, 6] [1, 2, 3, 4, 5, 6] 连接 [‘Hi!’] * 4 [‘Hi!’, ‘Hi!’, ‘Hi!’, ‘Hi!’] 复制 3 in [1, 2, 3] True 元素是否存在 for x in [1, 2, 3]: print(x, end=” “) 1 2 3 迭代 12345params = &#123;"server":"mpilgrim", "database":"master", "uid":"sa", "pwd":"secret"&#125;["%s=%s" % (k, v) for k, v in params.items()]['server=mpilgrim', 'uid=sa', 'database=master', 'pwd=secret']";".join(["%s=%s" % (k, v) for k, v in params.items()])'server=mpilgrim;uid=sa;database=master;pwd=secret' 13.字典（可变）12345678910a = &#123;"name":"yy", "age": 12&#125;#增a["name"] = "yy" #直接写key-value #删del a["name"]#改a["name"] = "zz" #相同key的值覆盖。#查：a["name"], a.get("name") for-else 12345#for中没有break,则else一定会执行for temp in strs: print(temp) else: print("") const 修改变量为不可变。字典常见操作 123456len(a) #键值对的个数a.keys() #["name","age"] (pyhton2) #dict_keys(["name","age"])(pyhton3)返回key的列表if "abc" in a.keys(): #判断是否存在某个key print("")a.values() #返回value的列表 14.元组(类似列表，不可变)列表可以增删改，元组不能改 1tup1 = (); #空元组 1234567&gt;&gt;&gt; tup1 = (50)&gt;&gt;&gt; type(tup1) # 不加逗号，类型为整型&lt;class 'int'&gt;&gt;&gt;&gt; tup1 = (50,)&gt;&gt;&gt; type(tup1) # 加上逗号，类型为元组&lt;class 'tuple'&gt; 访问： 12345tup1 = ('Google', 'Runoob', 1997, 2000)tup2 = (1, 2, 3, 4, 5, 6, 7 )print ("tup1[0]: ", tup1[0]) # Googleprint ("tup2[1:5]: ", tup2[1:5]) #(2, 3, 4, 5) 修改： 123456789tup1 = (12, 34.56);tup2 = ('abc', 'xyz')# 以下修改元组元素操作是非法的。# tup1[0] = 100# 创建一个新的元组tup3 = tup1 + tup2;print (tup3) #(12, 34.56, 'abc', 'xyz') 删除 1234tup = ('Google', 'Runoob', 1997, 2000)print (tup)del tup; 运算符 表达式 结果 描述 len((1, 2, 3)) 3 计算元素个数 (1, 2, 3) + (4, 5, 6) (1, 2, 3, 4, 5, 6) 连接 (‘Hi!’) * 4 (‘Hi!’, ‘Hi!’, ‘Hi!’, ‘Hi!’) 复制 3 in (1, 2, 3) True 元素是否存在 for x in (1, 2, 3): print x, 1 2 3 迭代 类似拆包 123456789101112131415161718a = (11,12)b = ab #out (11,12)c,d = a #类似拆包c #out 11d #out 12#例如info = &#123;"name":"ysw", "age":24&#125;for temp in info: print(temp)#("name":"ysw")#("age":24)for temp in info.items(): print("key=%s,value=%s"%(temp[0],temp[1]))#orfor a,b in info.items(): print("key=%s,value=%s"%(a,b)) 遍历技巧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445#在字典中遍历时，关键字和对应的值可以使用 items() 方法同时解读出来knights = &#123;'gallahad': 'the pure', 'robin': 'the brave'&#125;for k, v in knights.items(): print(k, v) #gallahad the pure #robin the brave#在序列中遍历时，索引位置和对应值可以使用 enumerate() 函数同时得到&gt;&gt;&gt; for i, v in enumerate(['tic', 'tac', 'toe']):... print(i, v)...0 tic1 tac2 toe#同时遍历两个或更多的序列，可以使用 zip() 组合&gt;&gt;&gt; questions = ['name', 'quest', 'favorite color']&gt;&gt;&gt; answers = ['lancelot', 'the holy grail', 'blue']&gt;&gt;&gt; for q, a in zip(questions, answers):... print('What is your &#123;0&#125;? It is &#123;1&#125;.'.format(q, a))...What is your name? It is lancelot.What is your quest? It is the holy grail.What is your favorite color? It is blue.#要反向遍历一个序列，首先指定这个序列，然后调用 reversed() 函数&gt;&gt;&gt; for i in reversed(range(1, 10, 2)):... print(i)...97531#要按顺序遍历一个序列，使用 sorted() 函数返回一个已排序的序列，并不修改原值&gt;&gt;&gt; basket = ['apple', 'orange', 'apple', 'pear', 'orange', 'banana']&gt;&gt;&gt; for f in sorted(set(basket)):... print(f)...applebananaorangepear 15.函数12345def abc(): print("") abc()#注意： 函数的定义需要在函数的调用之前，否则报错。 可更改与不可更改对象：在python中，strings,tuples和numbers是不可更改对象，list.dict等则是可更改对象。 不可变类型：变量赋值 a=5后在赋值a=10,生成了新的对象，原对象丢弃。 可变类型：变量赋值 la=[1,2,3,4] 后再赋值 la[2]=5 则是将 list la 的第三个元素值更改，本身la没有动，只是其内部的一部分值被修改了。 python 函数的参数传递： 不可变类型:类似 c++ 的值传递，如 整数、字符串、元组。如fun（a），传递的只是a的值，没有影响a对象本身。比如在 fun（a）内部修改 a 的值，只是修改另一个复制的对象，不会影响 a 本身 可变类型：类似 c++ 的引用传递，如 列表，字典。如 fun（la），则是将 la 真正的传过去，修改后fun外部的la也会受影响 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#默认函数参数def printinfo( name, age = 35 ): "打印任何传入的字符串" print ("名字: ", name); print ("年龄: ", age); return; printinfo( age=50, name="runoob" );print ("------------------------")printinfo( name="runoob" );#名字: runoob#年龄: 50#------------------------#名字: runoob#年龄: 35#可变参数def printinfo( *vartuple ): "打印任何传入的参数" print ("输出: ") for var in vartuple: print (var) return;printinfo(10,32,22)#匿名函数(lambda创建匿名函数)sum = lambda arg1, arg2: arg1 + arg2; print ("相加后的值为 : ", sum( 10, 20 ))print ("相加后的值为 : ", sum( 20, 20 ))#global关键字修改外部作用域变量num = 1def fun1(): global num # 需要使用 global 关键字声明 print(num) num = 123 print(num)fun1()#1#123#nonlocal 修改嵌套作用域变量def outer(): num = 10 def inner(): nonlocal num # nonlocal关键字声明 num = 100 print(num) inner() print(num)outer()#100#100 16.迭代器和生成器字符串，列表和元组对象都可用于创建迭代器。12345678910111213141516list = [1,2,3,4]it = iter(list)print(next(it)) #1print(next(it)) #2#遍历#1for x in it: print(x, end = "") # 1 2 3 4 #2while True: try: print(next(it)) except StopIteration: sys.exit() 生成器(TODO: 待理解)yield的函数，生成器是返回迭代器的函数，只能用于迭代操作。123456789101112131415161718import sys def fibonacci(n): # 生成器函数 - 斐波那契 a, b, counter = 0, 1, 0 while True: if (counter &gt; n): return yield a a, b = b, a + b counter += 1f = fibonacci(10) # f 是一个迭代器，由生成器返回生成 while True: try: print (next(f), end=" ") except StopIteration: sys.exit()# 0 1 1 2 3 5 8 13 21 34 55 17.模块为此 Python 提供了一个办法，把这些定义存放在文件中，为一些脚本或者交互式的解释器实例使用，这个文件被称为模块。 import语句，想使用 Python 源文件，只需在另一个源文件里执行 import 语句。 123456#!/usr/bin/python3# Filename: support.pydef print_func( par ): print ("Hello : ", par) return 12345678#!/usr/bin/python3# Filename: test.py # 导入模块import support #文件名 # 现在可以调用模块里包含的函数了support.print_func("Runoob") from…import语句,从模块中导入一个指定的部分到当前命名空间中。 123#导入模块 fibo 的 fib 函数from fibo import fib, fib2&gt;&gt;&gt; fib(500) From…import* 语句，把一个模块的所有内容全都导入到当前的命名空间。 模块除了方法定义，还可以包括可执行的代码。这些代码一般用来初始化这个模块。这些代码只有在第一次被导入时才会被执行。 name属性,一个模块被另一个程序第一次引入时，其主程序将运行。如果我们想在模块被引入时，模块中的某一程序块不执行,可以用name属性。 1234567#!/usr/bin/python3# Filename: using_name.pyif __name__ == '__main__': print('程序自身在运行')else: print('我来自另一模块') Python 会根据 sys.path 中的目录来寻找这个包中包含的子目录。 目录只有包含一个叫做 init.py 的文件才会被认作是一个包，主要是为了避免一些滥俗的名字（比如叫做 string）不小心的影响搜索路径中的有效模块。 推荐：from Package import specific_submodule 18.文件open()方法返回文件，第二个参数为文件打开方式。默认只读r。w写，a追加… 12345f = open("/tmp/test.txt","w")f.write("人生苦短，我用python！")f.close() f.read(size) 读取文件内容，size为空或负数则全部返回。 f.readline() f.readline() 会从文件中读取单独的一行。换行符为 ‘\n’。f.readline() 如果返回一个空字符串, 说明已经已经读取到最后一行。 f.readlines() 读取文件所有行，并以列表返回。 f.write(string) 将 string 写入到文件中, 然后返回写入的字符数。如果要写入一些不是字符串的东西, 那么将需要先进行转换。 19.类 类方法 123456789class MyClass: i = 12345 #类方法必须有一个额外的第一个参数，惯例是self，不固定；代表的的类的实例而非类 def f(self): return "hello world"x = MyClass()print("MyClass 类的属性 i 为：", x.i)print("MyClass 类的方法 f 输出为：", x.f()) 构造方法 1234567class Complex: #构造方法 def __init__(self, realpart, imagpart): self.r = realpart self.i = imagpartx = Complex(3.0, -4.5)print(x.r, x.i) # 输出结果：3.0 -4.5 123456789101112131415161718#类定义class people: #定义基本属性 name = '' age = 0 #定义私有属性,私有属性在类外部无法直接进行访问 __weight = 0 #定义构造方法 def __init__(self,n,a,w): self.name = n self.age = a self.__weight = w def speak(self): print("%s 说: 我 %d 岁。" %(self.name,self.age)) # 实例化类p = people('runoob',10,30)p.speak() 继承 123456789101112131415161718192021222324252627282930313233343536373839404142434445#类定义class people: #定义基本属性 name = '' age = 0 #定义私有属性,私有属性在类外部无法直接进行访问 __weight = 0 #定义构造方法 def __init__(self,n,a,w): self.name = n self.age = a self.__weight = w def speak(self): print("%s 说: 我 %d 岁。" %(self.name,self.age)) #单继承示例class student(people): grade = '' def __init__(self,n,a,w,g): #调用父类的构函 people.__init__(self,n,a,w) self.grade = g #覆写父类的方法 def speak(self): print("%s 说: 我 %d 岁了，我在读 %d 年级"%(self.name,self.age,self.grade)) #另一个类，多重继承之前的准备class speaker(): topic = '' name = '' def __init__(self,n,t): self.name = n self.topic = t def speak(self): print("我叫 %s，我是一个演说家，我演讲的主题是 %s"%(self.name,self.topic)) #多重继承class sample(speaker,student): a ='' def __init__(self,n,a,w,g,t): student.__init__(self,n,a,w,g) speaker.__init__(self,n,t) test = sample("Tim",25,80,4,"Python")test.speak() #方法名同，默认调用的是在括号中排前地父类的方法 20.正则表达式 re.match与re.search的区别re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 123456789101112131415161718import reline = "Cats are smarter than dogs";matchObj = re.match( r'dogs', line, re.M|re.I)if matchObj: print ("match --&gt; matchObj.group() : ", matchObj.group())else: print ("No match!!")matchObj = re.search( r'dogs', line, re.M|re.I)if matchObj: print ("search --&gt; matchObj.group() : ", matchObj.group())else: print ("No match!!") #No match!!#search --&gt; matchObj.group() : dogs 检索和替换 1234567891011121314import rephone = "2004-959-559 # 这是一个电话号码"# 删除注释num = re.sub(r'#.*$', "", phone)print ("电话号码 : ", num)# 移除非数字的内容num = re.sub(r'\D', "", phone)print ("电话号码 : ", num)#电话号码 : 2004-959-559 #电话号码 : 2004959559]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
